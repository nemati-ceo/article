{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056f7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall paperflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44489b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    " !pip install paperflow[extraction-all,rag,providers] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paperflow\n",
    "paperflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df28714",
   "metadata": {},
   "source": [
    "# PaperFlow: Complete Academic Paper Processing Demo\n",
    "\n",
    "PaperFlow is a unified pipeline for academic paper ingestion, extraction, and RAG (Retrieval-Augmented Generation). It allows you to:\n",
    "\n",
    "- **Search** across multiple academic sources (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- **Download** PDFs automatically\n",
    "- **Extract** text and structure from PDFs\n",
    "- **Chunk** content for RAG applications\n",
    "- **Embed** and store in vector databases\n",
    "- **Query** papers using natural language\n",
    "\n",
    "## Features\n",
    "\n",
    "- ğŸ” Multi-source search (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- ğŸ“¥ Automatic PDF downloading\n",
    "- ğŸ“„ Advanced PDF text extraction (Marker AI, Docling, MarkItDown)\n",
    "- âœ‚ï¸ Intelligent text chunking\n",
    "- ğŸ§  Vector embeddings for RAG\n",
    "- ğŸ’¾ ChromaDB integration\n",
    "- ğŸ”— LangChain compatibility\n",
    "- ğŸ“Š Tabular result display\n",
    "- ğŸ’» Command-line interface\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the latest version of PaperFlow with all optional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have CUDA GPU and want faster extraction\n",
    "PDF_DIR = './test_pdfs'\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = PaperPipeline(gpu=USE_GPU, pdf_dir=PDF_DIR)\n",
    "\n",
    "# Search for papers\n",
    "print('Searching for papers on transformers...')\n",
    "results = pipeline.search('transformer attention', sources=['pubmed'], max_results=3)\n",
    "\n",
    "# Display results\n",
    "print(results)\n",
    "print()\n",
    "\n",
    "# Download all papers first\n",
    "print('Downloading all papers...')\n",
    "papers = []\n",
    "for i, paper_meta in enumerate(results.papers, 1):\n",
    "    print(f'Downloading paper {i}: {paper_meta[\"title\"][:50]}...')\n",
    "    paper = pipeline.process(paper_meta, download=True, extract=False, chunk=False, embed=False)\n",
    "    papers.append(paper)\n",
    "    print(f'  - PDF saved: {paper.pdf_path}')\n",
    "    print()\n",
    "\n",
    "# Then extract all papers\n",
    "print('Extracting all papers...')\n",
    "for i, paper in enumerate(papers, 1):\n",
    "    print(f'Extracting paper {i}: {paper.metadata.title[:50]}...')\n",
    "    paper = pipeline.process(paper, download=False, extract=True, chunk=True, embed=False)\n",
    "    print(f'  - Sections: {len(paper.sections)}, Chunks: {len(paper.chunks)}')\n",
    "    print()\n",
    "\n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80d63ed",
   "metadata": {},
   "source": [
    "## What Happens Next?\n",
    "\n",
    "After processing, you can:\n",
    "\n",
    "- **Query the papers**: Use RAG to ask questions about the content\n",
    "- **Export to LangChain**: Get LangChain documents for further processing\n",
    "- **Save to vector database**: Store embeddings for semantic search\n",
    "- **Analyze content**: Access extracted sections, chunks, and metadata\n",
    "\n",
    "## Command Line Usage\n",
    "\n",
    "You can also use paperflow from the command line:\n",
    "\n",
    "```bash\n",
    "# Install with CLI support\n",
    "pip install paperflow\n",
    "\n",
    "# Search and display results\n",
    "paperflow \"transformer attention\" --sources pubmed --max-results 5\n",
    "```\n",
    "\n",
    "## Advanced Usage\n",
    "\n",
    "For more advanced features, install optional dependencies:\n",
    "\n",
    "```bash\n",
    "pip install paperflow[all]  # Full installation\n",
    "pip install paperflow[extraction]  # PDF extraction only\n",
    "pip install paperflow[rag]  # RAG features only\n",
    "```\n",
    "\n",
    "Check out the [documentation](https://github.com/osllmai/paperflow) for more examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b7c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have CUDA GPU and want faster extraction\n",
    "PDF_DIR = './test_pdfs'\n",
    "\n",
    "# Choose PDF extraction backend:\n",
    "# - \"auto\": Try marker â†’ docling â†’ markitdown (recommended)\n",
    "# - \"marker\": High quality, best for academic papers, GPU support\n",
    "# - \"docling\": Good table/figure extraction, IBM, GPU support  \n",
    "# - \"markitdown\": Lightweight, fast, CPU only, Microsoft\n",
    "EXTRACTION_BACKEND = \"auto\"\n",
    "\n",
    "print(f\"GPU acceleration: {'Enabled' if USE_GPU else 'Disabled'}\")\n",
    "print(f\"PDF directory: {PDF_DIR}\")\n",
    "print(f\"Extraction backend: {EXTRACTION_BACKEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ec56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Create pipeline with selected backend\n",
    "pipeline = PaperPipeline(\n",
    "    gpu=USE_GPU, \n",
    "    pdf_dir=PDF_DIR,\n",
    "    extraction_backend=EXTRACTION_BACKEND\n",
    ")\n",
    "\n",
    "print(f\"âœ… Pipeline created with {pipeline._extractor.active_backend} backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a02520b",
   "metadata": {},
   "source": [
    "## 1. arXiv Provider Demo\n",
    "\n",
    "Search for computer science papers on arXiv, the preprint server for physics, mathematics, computer science, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f7d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search arXiv\n",
    "print('ğŸ” Searching arXiv for \"transformer attention\" papers...')\n",
    "results = pipeline.search('transformer attention mechanism', sources=['arxiv'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {results.total_found} papers in {results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99883496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results\n",
    "if results.papers:\n",
    "    print(\"\\nğŸ“‹ arXiv Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âŒ No papers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb43037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results in JSON format\n",
    "if results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ arXiv Search Results in JSON:\")\n",
    "    print(json.dumps(results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4eff04",
   "metadata": {},
   "source": [
    "## 2. PubMed Provider Demo\n",
    "\n",
    "Search for biomedical research papers in PubMed, the premier database for biomedical literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search PubMed\n",
    "print('ğŸ” Searching PubMed for \"CRISPR gene editing\" papers...')\n",
    "pubmed_results = pipeline.search('CRISPR gene editing therapy', sources=['pubmed'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {pubmed_results.total_found} papers in {pubmed_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results\n",
    "if pubmed_results.papers:\n",
    "    print(\"\\nğŸ“‹ PubMed Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(pubmed_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7935d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results in JSON format\n",
    "if pubmed_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ PubMed Search Results in JSON:\")\n",
    "    print(json.dumps(pubmed_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9bbfcf",
   "metadata": {},
   "source": [
    "## 3. Semantic Scholar Provider Demo\n",
    "\n",
    "Search using Semantic Scholar's AI-powered academic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97efff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Semantic Scholar\n",
    "print('ğŸ” Searching Semantic Scholar for \"large language models\"...')\n",
    "sem_results = pipeline.search('large language models GPT', sources=['semantic_scholar'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {sem_results.total_found} papers in {sem_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedeb54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results\n",
    "if sem_results.papers:\n",
    "    print(\"\\nğŸ“‹ Semantic Scholar Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(sem_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5affa7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results in JSON format\n",
    "if sem_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ Semantic Scholar Search Results in JSON:\")\n",
    "    print(json.dumps(sem_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90213d8c",
   "metadata": {},
   "source": [
    "## 4. OpenAlex Provider Demo\n",
    "\n",
    "Search the OpenAlex catalog, which covers millions of scholarly works from all disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8490007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search OpenAlex\n",
    "print('ğŸ” Searching OpenAlex for \"climate change adaptation\"...')\n",
    "openalex_results = pipeline.search('climate change adaptation strategies', sources=['openalex'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {openalex_results.total_found} papers in {openalex_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results\n",
    "if openalex_results.papers:\n",
    "    print(\"\\nğŸ“‹ OpenAlex Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(openalex_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results in JSON format\n",
    "if openalex_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ OpenAlex Search Results in JSON:\")\n",
    "    print(json.dumps(openalex_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2b794",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline Demo\n",
    "\n",
    "Demonstrate the full pipeline: search â†’ download â†’ extract â†’ chunk â†’ embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fe0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline demonstration\n",
    "print(\"ğŸš€ Running complete pipeline...\")\n",
    "\n",
    "# 1. Search\n",
    "print(\"1. Searching for papers...\")\n",
    "search_results = pipeline.search(\"neural networks\", sources=[\"arxiv\"], max_results=1)\n",
    "\n",
    "if search_results.papers:\n",
    "    paper_dict = search_results.papers[0]\n",
    "    \n",
    "    # 2. Download\n",
    "    print(\"2. Downloading PDF...\")\n",
    "    paper = pipeline.download(paper_dict)\n",
    "    \n",
    "    # 3. Extract\n",
    "    print(\"3. Extracting content...\")\n",
    "    paper = pipeline.extract(paper)\n",
    "    \n",
    "    # 4. Chunk\n",
    "    print(\"4. Creating chunks...\")\n",
    "    paper = pipeline.chunk(paper)\n",
    "    \n",
    "    # 5. Embed (if embeddings available)\n",
    "    print(\"5. Creating embeddings...\")\n",
    "    try:\n",
    "        paper = pipeline.embed(paper)\n",
    "        print(\"âœ… Pipeline completed successfully!\")\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "        print(f\"Has embeddings: {paper.has_embeddings}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Embedding failed (missing dependencies): {e}\")\n",
    "        print(\"âœ… Pipeline completed (without embeddings)\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "else:\n",
    "    print(\"âŒ No papers found for pipeline demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742280e4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "âœ… **All 4 academic providers**: arXiv, PubMed, Semantic Scholar, OpenAlex  \n",
    "âœ… **Search across all sources** with formatted and JSON output  \n",
    "âœ… **Complete processing pipeline**: search â†’ download â†’ extract â†’ chunk â†’ embed  \n",
    "âœ… **PDF extraction backends**: Marker AI, Docling, MarkItDown with auto-fallback  \n",
    "âœ… **GPU acceleration** support for faster processing  \n",
    "\n",
    "### Key Features Used:\n",
    "- Unified search interface across all providers\n",
    "- Automatic PDF downloading and text extraction\n",
    "- Intelligent text chunking for RAG\n",
    "- Vector embeddings for semantic search\n",
    "- Tabular and JSON result display\n",
    "- Error handling and graceful fallbacks\n",
    "\n",
    "Happy researching! ğŸ”¬ğŸ“š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
