{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://claude.ai/chat/2635b704-20b2-4544-9801-76e006c43fe4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found these model folders:\n",
            "- .locks\n",
            "- models--bert-base-uncased\n",
            "- models--ds4sd--docling-layout-old\n",
            "- models--ds4sd--docling-models\n",
            "- models--facebook--sam-3d-body-dinov3\n",
            "- models--jetjodh--sam-3d-body-dinov3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# standard path\n",
        "cache_path = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
        "\n",
        "if cache_path.exists():\n",
        "    print(\"Found these model folders:\")\n",
        "    for item in os.listdir(cache_path):\n",
        "        print(f\"- {item}\")\n",
        "else:\n",
        "    print(f\"Directory not found at {cache_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipywidgets) (9.7.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
            "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: colorama>=0.4.4 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
            "Requirement already satisfied: decorator>=4.3.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.18.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments>=2.11.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data>=0.6.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure_eval in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
            "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
            "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
            "   ---------------------------------------- 0.0/914.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 914.9/914.9 kB 13.9 MB/s  0:00:00\n",
            "Downloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
            "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 2.2/2.2 MB 10.3 MB/s  0:00:00\n",
            "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
            "\n",
            "   -------------------------- ------------- 2/3 [ipywidgets]\n",
            "   ---------------------------------------- 3/3 [ipywidgets]\n",
            "\n",
            "Successfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n"
          ]
        }
      ],
      "source": [
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8JE3aBvTHHL",
        "outputId": "36138a9d-315c-4c2e-9673-d2133c531ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Requirement already satisfied: marker-pdf in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (1.10.1)\n",
            "Requirement already satisfied: arxiv in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: pymed in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (0.8.9)\n",
            "Requirement already satisfied: requests in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (2.32.5)\n",
            "Requirement already satisfied: torch in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (2.9.1)\n",
            "Requirement already satisfied: biopython in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (1.86)\n",
            "Requirement already satisfied: Pillow<11.0.0,>=10.1.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (10.4.0)\n",
            "Requirement already satisfied: anthropic<0.47.0,>=0.46.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (0.46.0)\n",
            "Requirement already satisfied: click<9.0.0,>=8.2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (8.3.1)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from marker-pdf) (1.2.0)\n",
            "Requirement already satisfied: ftfy<7.0.0,>=6.1.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (6.3.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (1.56.0)\n",
            "Requirement already satisfied: markdown2<3.0.0,>=2.5.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (2.5.4)\n",
            "Requirement already satisfied: markdownify<2.0.0,>=1.1.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (1.2.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.65.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (1.109.1)\n",
            "Requirement already satisfied: pdftext<0.7.0,>=0.6.3 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (0.6.3)\n",
            "Requirement already satisfied: pre-commit<5.0.0,>=4.2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (4.5.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (2.12.5)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.0.3 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from marker-pdf) (2.10.1)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (1.2.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (3.14.3)\n",
            "Requirement already satisfied: regex<2025.0.0,>=2024.4.28 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.6.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (1.7.1)\n",
            "Requirement already satisfied: surya-ocr<0.18.0,>=0.17.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (0.17.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.45.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from marker-pdf) (4.57.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from torch) (3.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from torch) (2025.12.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from torch) (80.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (0.12.0)\n",
            "Requirement already satisfied: sniffio in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (3.11)\n",
            "Requirement already satisfied: colorama in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from click<9.0.0,>=8.2.0->marker-pdf) (0.4.6)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from ftfy<7.0.0,>=6.1.1->marker-pdf) (0.2.13)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.0.0->marker-pdf) (2.45.0)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from google-genai<2.0.0,>=1.0.0->marker-pdf) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from google-genai<2.0.0,>=1.0.0->marker-pdf) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests) (2.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.0.0->marker-pdf) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.0.0->marker-pdf) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.0.0->marker-pdf) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (0.16.0)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from markdownify<2.0.0,>=1.1.0->marker-pdf) (4.14.2)\n",
            "Requirement already satisfied: six<2,>=1.15 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from markdownify<2.0.0,>=1.1.0->marker-pdf) (1.17.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from beautifulsoup4<5,>=4.9->markdownify<2.0.0,>=1.1.0->marker-pdf) (2.5)\n",
            "Requirement already satisfied: pypdfium2==4.30.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pdftext<0.7.0,>=0.6.3->marker-pdf) (4.30.0)\n",
            "Requirement already satisfied: cfgv>=2.0.0 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (3.4.0)\n",
            "Requirement already satisfied: identify>=1.0.0 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (2.6.10)\n",
            "Requirement already satisfied: nodeenv>=0.11.1 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (1.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (6.0.3)\n",
            "Requirement already satisfied: virtualenv>=20.10.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (20.35.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (2.41.5)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (0.4.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai<2.0.0,>=1.0.0->marker-pdf) (0.6.1)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (2.3.5)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (3.5.0)\n",
            "Requirement already satisfied: einops<0.9.0,>=0.8.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from surya-ocr<0.18.0,>=0.17.0->marker-pdf) (0.8.1)\n",
            "Requirement already satisfied: opencv-python-headless==4.11.0.86 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from surya-ocr<0.18.0,>=0.17.0->marker-pdf) (4.11.0.86)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.3.6 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from surya-ocr<0.18.0,>=0.17.0->marker-pdf) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (25.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (0.7.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from arxiv) (6.0.12)\n",
            "Requirement already satisfied: sgmllib3k in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from virtualenv>=20.10.0->pre-commit<5.0.0,>=4.2.0->marker-pdf) (0.3.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install marker-pdf arxiv pymed requests torch biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6yHE3a0XK_R",
        "outputId": "7d8b49d2-3301-41ea-d92f-78cfcd327c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Loading Marker AI models...\n",
            "‚úÖ Marker models loaded.\n"
          ]
        }
      ],
      "source": [
        "# --- MARKER AI SETUP ---\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "print(\"‚è≥ Loading Marker AI models...\")\n",
        "\n",
        "try:\n",
        "    from marker.converters.pdf import PdfConverter\n",
        "    from marker.models import create_model_dict\n",
        "    from marker.output import text_from_rendered\n",
        "\n",
        "    converter = PdfConverter(\n",
        "        artifact_dict=create_model_dict(),\n",
        "    )\n",
        "    print(\"‚úÖ Marker models loaded.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"‚ùå ImportError while importing Marker.\")\n",
        "    print(f\"   Python: {sys.executable}\")\n",
        "    print(f\"   Error : {e}\")\n",
        "    print(\"\\n‚úÖ Fix:\")\n",
        "    print(\"   1) Activate your env:  conda activate article\")\n",
        "    print(\"   2) Install correct pkg: python -m pip install -U marker-pdf\")\n",
        "    print(\"\\nüîé Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "    raise  # fail loudly so you see the real reason\n",
        "\n",
        "except Exception:\n",
        "    print(\"‚ùå Model Load Error (non-import).\")\n",
        "    print(f\"   Python: {sys.executable}\")\n",
        "    print(\"\\nüîé Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Scanning loaded models for file paths...\n",
            "\n",
            "üì¶ Model Key: layout_model\n",
            "üìÇ Location:  Path not found\n",
            "\n",
            "üì¶ Model Key: recognition_model\n",
            "üìÇ Location:  Path not found\n",
            "\n",
            "üì¶ Model Key: table_rec_model\n",
            "üìÇ Location:  C:\\Users\\llmserver\\AppData\\Local\\datalab\\datalab\\Cache\\models\\table_recognition/2025_02_18\n",
            "\n",
            "üì¶ Model Key: detection_model\n",
            "üìÇ Location:  C:\\Users\\llmserver\\AppData\\Local\\datalab\\datalab\\Cache\\models\\text_detection/2025_05_07\n",
            "\n",
            "üì¶ Model Key: ocr_error_model\n",
            "üìÇ Location:  C:\\Users\\llmserver\\AppData\\Local\\datalab\\datalab\\Cache\\models\\ocr_error_detection/2025_02_18\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from marker.models import create_model_dict\n",
        "\n",
        "model_dict = create_model_dict()\n",
        "\n",
        "print(\"üîç Scanning loaded models for file paths...\\n\")\n",
        "\n",
        "for key, model_obj in model_dict.items():\n",
        "    # Try to find the path in different possible locations within the object\n",
        "    path = \"Path not found\"\n",
        "    \n",
        "    # Check if it has a model attribute with a config\n",
        "    if hasattr(model_obj, 'model') and hasattr(model_obj.model, 'config'):\n",
        "        path = getattr(model_obj.model.config, '_name_or_path', \"Path not found\")\n",
        "    \n",
        "    # Check if the object itself has a config (common in some Marker versions)\n",
        "    elif hasattr(model_obj, 'config'):\n",
        "        path = getattr(model_obj.config, '_name_or_path', \"Path not found\")\n",
        "\n",
        "    print(f\"üì¶ Model Key: {key}\")\n",
        "    print(f\"üìÇ Location:  {path}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "import arxiv\n",
        "from Bio import Entrez\n",
        "from datetime import datetime, timedelta\n",
        "import uuid\n",
        "import json\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "MAX_RESULTS = 50\n",
        "DOWNLOAD_DELAY = 2\n",
        "NCBI_API_KEY = \"162cefdacd4448a08831092c05eab6e73a09\"\n",
        "NCBI_EMAIL = \"shakeri163@gmail.com\"\n",
        "\n",
        "# Folder structure\n",
        "PDF_FOLDER = \"papers_pdf\"\n",
        "MARKDOWN_FOLDER = \"papers_markdown\"\n",
        "METADATA_FILE = \"papers_metadata.json\"\n",
        "\n",
        "# arXiv Categories for filtering\n",
        "ARXIV_CATEGORIES = {\n",
        "    'cs.AI': 'Artificial Intelligence',\n",
        "    'cs.LG': 'Machine Learning',\n",
        "    'cs.CV': 'Computer Vision',\n",
        "    'cs.CL': 'Computation and Language',\n",
        "    'cs.NE': 'Neural and Evolutionary Computing',\n",
        "    'q-bio.GN': 'Genomics',\n",
        "    'q-bio.QM': 'Quantitative Methods',\n",
        "    'physics.bio-ph': 'Biological Physics',\n",
        "    'stat.ML': 'Machine Learning (Statistics)',\n",
        "    'math.ST': 'Statistics Theory'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# METADATA DATABASE\n",
        "# ---------------------------------------------------------\n",
        "class PaperDatabase:\n",
        "    \"\"\"Manages paper metadata with UUIDs and citations.\"\"\"\n",
        "    \n",
        "    def __init__(self, db_file=METADATA_FILE):\n",
        "        self.db_file = db_file\n",
        "        self.papers = self.load()\n",
        "    \n",
        "    def load(self):\n",
        "        \"\"\"Load existing metadata database.\"\"\"\n",
        "        if os.path.exists(self.db_file):\n",
        "            with open(self.db_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "    \n",
        "    def save(self):\n",
        "        \"\"\"Save metadata database.\"\"\"\n",
        "        with open(self.db_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.papers, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    def add_paper(self, metadata):\n",
        "        \"\"\"Add a paper with UUID and citation.\"\"\"\n",
        "        paper_uuid = str(uuid.uuid4())\n",
        "        \n",
        "        # Generate citation\n",
        "        citation = self.generate_citation(metadata)\n",
        "        \n",
        "        # Store complete metadata\n",
        "        self.papers[paper_uuid] = {\n",
        "            'uuid': paper_uuid,\n",
        "            'title': metadata['title'],\n",
        "            'authors': metadata['authors'],\n",
        "            'year': metadata.get('year', 'N/A'),\n",
        "            'source': metadata['source'],\n",
        "            'arxiv_id': metadata.get('arxiv_id', 'N/A'),\n",
        "            'doi': metadata.get('doi', 'N/A'),\n",
        "            'url': metadata['url'],\n",
        "            'pdf_path': metadata.get('pdf_path', 'N/A'),\n",
        "            'markdown_path': metadata.get('markdown_path', 'N/A'),\n",
        "            'citation': citation,\n",
        "            'categories': metadata.get('categories', []),\n",
        "            'abstract': metadata.get('abstract', ''),\n",
        "            'added_date': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        self.save()\n",
        "        return paper_uuid, citation\n",
        "    \n",
        "    def generate_citation(self, metadata):\n",
        "        \"\"\"Generate APA-style citation.\"\"\"\n",
        "        authors = metadata['authors']\n",
        "        if isinstance(authors, list):\n",
        "            if len(authors) == 1:\n",
        "                author_str = authors[0]\n",
        "            elif len(authors) == 2:\n",
        "                author_str = f\"{authors[0]} & {authors[1]}\"\n",
        "            elif len(authors) > 2:\n",
        "                author_str = f\"{authors[0]} et al.\"\n",
        "            else:\n",
        "                author_str = \"Unknown\"\n",
        "        else:\n",
        "            author_str = authors\n",
        "        \n",
        "        year = metadata.get('year', 'n.d.')\n",
        "        title = metadata['title']\n",
        "        \n",
        "        if metadata['source'] == 'arXiv':\n",
        "            arxiv_id = metadata.get('arxiv_id', '')\n",
        "            citation = f\"{author_str} ({year}). {title}. arXiv preprint arXiv:{arxiv_id}.\"\n",
        "        else:\n",
        "            citation = f\"{author_str} ({year}). {title}. {metadata['source']}.\"\n",
        "        \n",
        "        return citation\n",
        "    \n",
        "    def get_paper(self, paper_uuid):\n",
        "        \"\"\"Retrieve paper by UUID.\"\"\"\n",
        "        return self.papers.get(paper_uuid)\n",
        "    \n",
        "    def list_papers(self):\n",
        "        \"\"\"List all papers with UUIDs.\"\"\"\n",
        "        return self.papers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# EXTRACT COMPREHENSIVE ARXIV METADATA\n",
        "# ---------------------------------------------------------\n",
        "def extract_arxiv_metadata(paper):\n",
        "    \"\"\"Extract all available metadata from arXiv paper.\"\"\"\n",
        "    metadata = {\n",
        "        'title': paper.title,\n",
        "        'authors': [a.name for a in paper.authors],\n",
        "        'abstract': paper.summary,\n",
        "        'year': paper.published.year,\n",
        "        'published': paper.published.strftime('%Y-%m-%d'),\n",
        "        'updated': paper.updated.strftime('%Y-%m-%d') if paper.updated else 'N/A',\n",
        "        'arxiv_id': paper.get_short_id(),\n",
        "        'entry_id': paper.entry_id,\n",
        "        'doi': paper.doi if paper.doi else 'N/A',\n",
        "        'primary_category': paper.primary_category,\n",
        "        'categories': paper.categories,\n",
        "        'comment': paper.comment if paper.comment else 'N/A',\n",
        "        'journal_ref': paper.journal_ref if paper.journal_ref else 'N/A',\n",
        "        'pdf_url': paper.pdf_url,\n",
        "        'url': paper.entry_id,\n",
        "        'source': 'arXiv'\n",
        "    }\n",
        "    \n",
        "    return metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# PDF DOWNLOAD AND ORGANIZATION\n",
        "# ---------------------------------------------------------\n",
        "def download_and_save_pdf(paper, source_type, db):\n",
        "    \"\"\"Download PDF and save with organized naming.\"\"\"\n",
        "    os.makedirs(PDF_FOLDER, exist_ok=True)\n",
        "    \n",
        "    if source_type == '1':  # arXiv\n",
        "        metadata = extract_arxiv_metadata(paper)\n",
        "        safe_name = sanitize_filename(paper.title)\n",
        "        pdf_filename = f\"{metadata['arxiv_id']}_{safe_name}.pdf\"\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_filename)\n",
        "        \n",
        "        print(f\"   ‚¨áÔ∏è Downloading: {paper.title[:50]}...\")\n",
        "        try:\n",
        "            paper.download_pdf(filename=pdf_path)\n",
        "            print(f\"   ‚úÖ PDF saved: {pdf_path}\")\n",
        "            metadata['pdf_path'] = pdf_path\n",
        "            \n",
        "            # Add to database\n",
        "            paper_uuid, citation = db.add_paper(metadata)\n",
        "            print(f\"   üÜî UUID: {paper_uuid}\")\n",
        "            print(f\"   üìù Citation: {citation[:80]}...\")\n",
        "            \n",
        "            return True, paper_uuid\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Download failed: {e}\")\n",
        "            return False, None\n",
        "            \n",
        "    else:  # PubMed\n",
        "        metadata = {\n",
        "            'title': paper['title'],\n",
        "            'authors': paper['authors'][:3] if paper['authors'] else ['Unknown'],\n",
        "            'year': str(paper['date'])[:4] if paper['date'] else 'N/A',\n",
        "            'source': 'PubMed Central',\n",
        "            'url': f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{paper['pmc_id']}/\",\n",
        "            'pmc_id': paper['pmc_id'],\n",
        "            'abstract': get_pubmed_abstract(paper['pmc_id'])\n",
        "        }\n",
        "        \n",
        "        safe_name = sanitize_filename(paper['title'])\n",
        "        pdf_filename = f\"{paper['pmc_id']}_{safe_name}.pdf\"\n",
        "        pdf_path = os.path.join(PDF_FOLDER, pdf_filename)\n",
        "        \n",
        "        print(f\"   ‚¨áÔ∏è Downloading: {paper['title'][:50]}...\")\n",
        "        \n",
        "        if download_pubmed_pdf(paper['pmc_id'], pdf_path):\n",
        "            print(f\"   ‚úÖ PDF saved: {pdf_path}\")\n",
        "            metadata['pdf_path'] = pdf_path\n",
        "            \n",
        "            # Add to database\n",
        "            paper_uuid, citation = db.add_paper(metadata)\n",
        "            print(f\"   üÜî UUID: {paper_uuid}\")\n",
        "            print(f\"   üìù Citation: {citation[:80]}...\")\n",
        "            \n",
        "            return True, paper_uuid\n",
        "        else:\n",
        "            return False, None\n",
        "\n",
        "def sanitize_filename(title):\n",
        "    \"\"\"Create safe filename from title.\"\"\"\n",
        "    safe = re.sub(r'[^\\w\\s-]', '', title)\n",
        "    safe = re.sub(r'[-\\s]+', '_', safe)\n",
        "    return safe[:50]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------\n",
        "# MARKER AI INITIALIZATION\n",
        "# ---------------------------------------------------------\n",
        "print(\"‚è≥ Loading Marker AI models...\")\n",
        "try:\n",
        "    from marker.converters.pdf import PdfConverter\n",
        "    from marker.models import create_model_dict\n",
        "    from marker.output import text_from_rendered\n",
        "    \n",
        "    # Initialize converter (loads PyTorch models)\n",
        "    converter = PdfConverter(\n",
        "        artifact_dict=create_model_dict(),\n",
        "    )\n",
        "    print(\"‚úÖ Marker models loaded.\")\n",
        "    MARKER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ùå Critical Error: Marker library outdated or missing.\")\n",
        "    print(\"Run: pip install marker-pdf --upgrade\")\n",
        "    MARKER_AVAILABLE = False\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Model Load Error: {e}\")\n",
        "    MARKER_AVAILABLE = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# SELECTIVE MARKDOWN EXTRACTION WITH MARKER AI\n",
        "# ---------------------------------------------------------\n",
        "def extract_sections_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extract only important sections using Marker AI: Abstract, Introduction, Conclusion.\n",
        "    \"\"\"\n",
        "    if not MARKER_AVAILABLE:\n",
        "        print(\"   ‚ö†Ô∏è Marker AI not available. Skipping extraction.\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        print(f\"   ü§ñ Processing with Marker AI...\")\n",
        "        \n",
        "        # Convert PDF to markdown using Marker AI\n",
        "        rendered = converter(pdf_path)\n",
        "        full_text, _, images = text_from_rendered(rendered)\n",
        "        \n",
        "        print(f\"   üìù Extracted {len(full_text)} characters\")\n",
        "        \n",
        "        # Find sections using regex patterns\n",
        "        sections = {\n",
        "            'abstract': extract_section(full_text, r'abstract', r'introduction|keywords|1\\s+introduction'),\n",
        "            'introduction': extract_section(full_text, r'introduction|1\\s+introduction', r'related work|methodology|method|background|2\\s+'),\n",
        "            'conclusion': extract_section(full_text, r'conclusion|conclusions|discussion and conclusion', r'references|acknowledgment|appendix|bibliography')\n",
        "        }\n",
        "        \n",
        "        # Clean up sections\n",
        "        for key in sections:\n",
        "            if sections[key]:\n",
        "                # Remove excessive whitespace and newlines\n",
        "                sections[key] = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', sections[key])\n",
        "                sections[key] = sections[key].strip()\n",
        "        \n",
        "        return sections\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è Marker AI extraction failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_section(text, start_pattern, end_pattern):\n",
        "    \"\"\"Extract text between section headers with improved patterns.\"\"\"\n",
        "    import re\n",
        "    \n",
        "    # Case insensitive search with word boundaries\n",
        "    start_match = re.search(r'\\b' + start_pattern + r'\\b', text, re.IGNORECASE)\n",
        "    if not start_match:\n",
        "        # Try without word boundaries for numbered sections\n",
        "        start_match = re.search(start_pattern, text, re.IGNORECASE)\n",
        "        if not start_match:\n",
        "            return \"\"\n",
        "    \n",
        "    start_pos = start_match.end()\n",
        "    \n",
        "    # Skip section number and title line\n",
        "    # Find first paragraph after header\n",
        "    lines = text[start_pos:].split('\\n')\n",
        "    actual_start = 0\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.strip() and len(line.strip()) > 20:  # First substantial line\n",
        "            actual_start = sum(len(l) + 1 for l in lines[:i])\n",
        "            break\n",
        "    \n",
        "    start_pos += actual_start\n",
        "    \n",
        "    # Find end of section\n",
        "    end_match = re.search(r'\\b' + end_pattern + r'\\b', text[start_pos:], re.IGNORECASE)\n",
        "    if not end_match:\n",
        "        end_match = re.search(end_pattern, text[start_pos:], re.IGNORECASE)\n",
        "    \n",
        "    if end_match:\n",
        "        end_pos = start_pos + end_match.start()\n",
        "        section_text = text[start_pos:end_pos]\n",
        "    else:\n",
        "        # Take next 3000 characters if no end found\n",
        "        section_text = text[start_pos:start_pos+3000]\n",
        "    \n",
        "    return section_text.strip()\n",
        "\n",
        "def create_selective_markdown(paper_uuid, metadata, sections):\n",
        "    \"\"\"Create markdown with only important sections.\"\"\"\n",
        "    os.makedirs(MARKDOWN_FOLDER, exist_ok=True)\n",
        "    \n",
        "    title = metadata['title']\n",
        "    safe_name = sanitize_filename(title)\n",
        "    md_filename = f\"{paper_uuid[:8]}_{safe_name}.md\"\n",
        "    md_path = os.path.join(MARKDOWN_FOLDER, md_filename)\n",
        "    \n",
        "    # Build markdown content\n",
        "    content = f\"\"\"# {title}\n",
        "\n",
        "**UUID:** `{paper_uuid}`\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Metadata\n",
        "\n",
        "| Field | Value |\n",
        "|-------|-------|\n",
        "| **Authors** | {', '.join(metadata['authors']) if isinstance(metadata['authors'], list) else metadata['authors']} |\n",
        "| **Year** | {metadata.get('year', 'N/A')} |\n",
        "| **Source** | {metadata['source']} |\n",
        "| **arXiv ID** | {metadata.get('arxiv_id', 'N/A')} |\n",
        "| **DOI** | {metadata.get('doi', 'N/A')} |\n",
        "| **Categories** | {', '.join(metadata.get('categories', [])) if isinstance(metadata.get('categories'), list) else 'N/A'} |\n",
        "\n",
        "**üîó URL:** [{metadata['url']}]({metadata['url']})\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Citation\n",
        "\n",
        "```\n",
        "{metadata['citation']}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Abstract\n",
        "\n",
        "{sections.get('abstract', metadata.get('abstract', 'Not available'))}\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Introduction\n",
        "\n",
        "{sections.get('introduction', '*Section not extracted or not found in document*')}\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Conclusion\n",
        "\n",
        "{sections.get('conclusion', '*Section not extracted or not found in document*')}\n",
        "\n",
        "---\n",
        "\n",
        "## üìé File Information\n",
        "\n",
        "- **PDF Location:** `{metadata.get('pdf_path', 'N/A')}`\n",
        "- **Extracted:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "- **Extraction Method:** Marker AI\n",
        "\n",
        "---\n",
        "\n",
        "*This markdown contains only key sections (Abstract, Introduction, Conclusion) for quick reference. See the full PDF for complete content including Methods, Results, Discussion, and References.*\n",
        "\"\"\"\n",
        "    \n",
        "    with open(md_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(content)\n",
        "    \n",
        "    return md_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# ARXIV HANDLERS\n",
        "# ---------------------------------------------------------\n",
        "def build_arxiv_query(topic, filters):\n",
        "    \"\"\"Build advanced arXiv query with filters.\"\"\"\n",
        "    query_parts = [topic]\n",
        "    \n",
        "    if filters.get('categories'):\n",
        "        cat_query = ' OR '.join([f'cat:{cat}' for cat in filters['categories']])\n",
        "        query_parts.append(f\"({cat_query})\")\n",
        "    \n",
        "    if filters.get('author'):\n",
        "        query_parts.append(f'au:{filters[\"author\"]}')\n",
        "    \n",
        "    if filters.get('title_contains'):\n",
        "        query_parts.append(f'ti:{filters[\"title_contains\"]}')\n",
        "    \n",
        "    return ' AND '.join(query_parts)\n",
        "\n",
        "def get_arxiv(query, filters=None):\n",
        "    \"\"\"Get arXiv papers with advanced filtering options.\"\"\"\n",
        "    if filters is None:\n",
        "        filters = {}\n",
        "    \n",
        "    search_query = build_arxiv_query(query, filters)\n",
        "    \n",
        "    sort_by = filters.get('sort_by', 'announced')\n",
        "    if sort_by == 'announced':\n",
        "        sort_criterion = arxiv.SortCriterion.LastUpdatedDate\n",
        "    elif sort_by == 'submitted':\n",
        "        sort_criterion = arxiv.SortCriterion.SubmittedDate\n",
        "    else:\n",
        "        sort_criterion = arxiv.SortCriterion.Relevance\n",
        "    \n",
        "    max_results = filters.get('max_results', MAX_RESULTS)\n",
        "    \n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(\n",
        "        query=search_query,\n",
        "        max_results=max_results * 2,\n",
        "        sort_by=sort_criterion,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "    \n",
        "    papers = list(client.results(search))\n",
        "    \n",
        "    if filters.get('date_from') or filters.get('date_to'):\n",
        "        filtered_papers = []\n",
        "        for paper in papers:\n",
        "            paper_date = paper.published\n",
        "            \n",
        "            if filters.get('date_from') and paper_date < filters['date_from']:\n",
        "                continue\n",
        "            if filters.get('date_to') and paper_date > filters['date_to']:\n",
        "                continue\n",
        "                \n",
        "            filtered_papers.append(paper)\n",
        "        papers = filtered_papers\n",
        "    \n",
        "    return papers[:max_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# PUBMED HANDLERS\n",
        "# ---------------------------------------------------------\n",
        "def get_pubmed(query):\n",
        "    Entrez.email = NCBI_EMAIL\n",
        "    Entrez.api_key = NCBI_API_KEY\n",
        "\n",
        "    try:\n",
        "        handle = Entrez.esearch(db=\"pmc\", term=query, retmax=MAX_RESULTS)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        ids = record.get(\"IdList\", [])\n",
        "        if not ids:\n",
        "            return []\n",
        "\n",
        "        handle = Entrez.esummary(db=\"pmc\", id=\",\".join(ids))\n",
        "        summaries = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        papers = []\n",
        "        for summary in summaries:\n",
        "            papers.append({\n",
        "                'pmc_id': f\"PMC{summary.get('Id', '')}\",\n",
        "                'title': summary.get('Title', 'No title'),\n",
        "                'authors': summary.get('AuthorList', []),\n",
        "                'date': summary.get('PubDate', 'N/A'),\n",
        "                'source': summary.get('Source', ''),\n",
        "            })\n",
        "        return papers\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå PubMed API Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_pubmed_abstract(pmc_id):\n",
        "    \"\"\"Fetch abstract for a single PMC article.\"\"\"\n",
        "    try:\n",
        "        handle = Entrez.efetch(db=\"pmc\", id=pmc_id.replace(\"PMC\", \"\"), rettype=\"xml\")\n",
        "        content = handle.read()\n",
        "        handle.close()\n",
        "        if b'<abstract>' in content:\n",
        "            start = content.find(b'<abstract>') + 10\n",
        "            end = content.find(b'</abstract>')\n",
        "            abstract = content[start:end].decode('utf-8', errors='ignore')\n",
        "            abstract = re.sub(r'<[^>]+>', '', abstract).strip()\n",
        "            return abstract\n",
        "    except:\n",
        "        pass\n",
        "    return \"Abstract not available\"\n",
        "\n",
        "def download_pubmed_pdf(pmc_id, pdf_path):\n",
        "    \"\"\"Download PDF from PubMed Central.\"\"\"\n",
        "    import tarfile\n",
        "    import io\n",
        "    \n",
        "    oa_url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmc_id}\"\n",
        "    \n",
        "    try:\n",
        "        r = requests.get(oa_url, timeout=30)\n",
        "        if r.status_code != 200:\n",
        "            return False\n",
        "\n",
        "        tgz_match = re.search(r'href=\"(ftp://[^\"]+\\.tar\\.gz)\"', r.text)\n",
        "        if not tgz_match:\n",
        "            return False\n",
        "\n",
        "        tgz_url = tgz_match.group(1)\n",
        "        tgz_url = tgz_url.replace(\"ftp://ftp.ncbi.nlm.nih.gov/\", \"https://ftp.ncbi.nlm.nih.gov/\")\n",
        "\n",
        "        r2 = requests.get(tgz_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=120)\n",
        "        if r2.status_code != 200:\n",
        "            return False\n",
        "\n",
        "        tar_bytes = io.BytesIO(r2.content)\n",
        "        with tarfile.open(fileobj=tar_bytes, mode='r:gz') as tar:\n",
        "            for member in tar.getmembers():\n",
        "                if member.name.endswith('.pdf'):\n",
        "                    pdf_file = tar.extractfile(member)\n",
        "                    if pdf_file:\n",
        "                        with open(pdf_path, 'wb') as f:\n",
        "                            f.write(pdf_file.read())\n",
        "                        return True\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"      Error: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# DISPLAY HELPERS\n",
        "# ---------------------------------------------------------\n",
        "def display_results(papers, source_type):\n",
        "    print(\"\\n\" + \"=\" * 160)\n",
        "    if source_type == '1':\n",
        "        print(f\"{'#':<4} | {'arXiv ID':<15} | {'Published':<12} | {'Updated':<12} | {'Categories':<20} | {'Title':<60}\")\n",
        "    else:\n",
        "        print(f\"{'#':<4} | {'PMC ID':<15} | {'Date':<12} | {'Source':<30} | {'Title':<60}\")\n",
        "    print(\"=\" * 160)\n",
        "\n",
        "    for i, p in enumerate(papers):\n",
        "        if source_type == '1':\n",
        "            arxiv_id = p.get_short_id()\n",
        "            published = p.published.strftime('%Y-%m-%d')\n",
        "            updated = p.updated.strftime('%Y-%m-%d') if p.updated else 'N/A'\n",
        "            categories = ', '.join(p.categories[:2]) if len(p.categories) > 2 else ', '.join(p.categories)\n",
        "            if len(categories) > 18:\n",
        "                categories = categories[:15] + \"...\"\n",
        "            title = p.title.replace('\\n', ' ')\n",
        "            if len(title) > 57:\n",
        "                title = title[:54] + \"...\"\n",
        "            \n",
        "            print(f\"{i+1:<4} | {arxiv_id:<15} | {published:<12} | {updated:<12} | {categories:<20} | {title:<60}\")\n",
        "        else:\n",
        "            pmc_id = p['pmc_id']\n",
        "            date = str(p['date'])[:12] if p['date'] else \"N/A\"\n",
        "            source = p['source'][:28] if len(p['source']) > 28 else p['source']\n",
        "            title = p['title'].replace('\\n', ' ')\n",
        "            if len(title) > 57:\n",
        "                title = title[:54] + \"...\"\n",
        "            \n",
        "            print(f\"{i+1:<4} | {pmc_id:<15} | {date:<12} | {source:<30} | {title:<60}\")\n",
        "\n",
        "    print(\"=\" * 160 + \"\\n\")\n",
        "\n",
        "def get_arxiv_filters():\n",
        "    \"\"\"Interactive filter selection for arXiv.\"\"\"\n",
        "    filters = {}\n",
        "    \n",
        "    print(\"\\n--- arXiv Filters (press Enter to skip) ---\")\n",
        "    \n",
        "    print(\"\\nSort by:\")\n",
        "    print(\"1. Latest Announcement (default)\")\n",
        "    print(\"2. Submission Date\")\n",
        "    print(\"3. Relevance\")\n",
        "    sort_choice = input(\"Choose (1-3): \").strip()\n",
        "    if sort_choice == '2':\n",
        "        filters['sort_by'] = 'submitted'\n",
        "    elif sort_choice == '3':\n",
        "        filters['sort_by'] = 'relevance'\n",
        "    else:\n",
        "        filters['sort_by'] = 'announced'\n",
        "    \n",
        "    print(\"\\nAvailable Categories:\")\n",
        "    for i, (code, name) in enumerate(ARXIV_CATEGORIES.items(), 1):\n",
        "        print(f\"{i}. {code} - {name}\")\n",
        "    \n",
        "    cat_input = input(\"\\nEnter category numbers (comma-separated, e.g., 1,2): \").strip()\n",
        "    if cat_input:\n",
        "        try:\n",
        "            indices = [int(x.strip()) - 1 for x in cat_input.split(',')]\n",
        "            cat_list = list(ARXIV_CATEGORIES.keys())\n",
        "            filters['categories'] = [cat_list[i] for i in indices if 0 <= i < len(cat_list)]\n",
        "        except:\n",
        "            print(\"Invalid input, skipping categories\")\n",
        "    \n",
        "    date_input = input(\"\\nLast N days (e.g., 7 for last week): \").strip()\n",
        "    if date_input:\n",
        "        try:\n",
        "            days = int(date_input)\n",
        "            filters['date_from'] = datetime.now() - timedelta(days=days)\n",
        "        except:\n",
        "            print(\"Invalid input, skipping date filter\")\n",
        "    \n",
        "    author = input(\"\\nFilter by author name: \").strip()\n",
        "    if author:\n",
        "        filters['author'] = author\n",
        "    \n",
        "    return filters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "  üìö PAPER DOWNLOAD & EXTRACTION SYSTEM V6 (Marker AI)\n",
            "================================================================================\n",
            "  üìÅ PDFs saved to: papers_pdf/\n",
            "  üìù Markdown saved to: papers_markdown/\n",
            "  üóÑÔ∏è  Metadata database: papers_metadata.json\n",
            "================================================================================\n",
            "\n",
            "1. arXiv\n",
            "2. PubMed (PMC Open Access)\n",
            "\n",
            "üîç Searching for 'llm' (Max 50)...\n",
            "\n",
            "================================================================================================================================================================\n",
            "#    | arXiv ID        | Published    | Updated      | Categories           | Title                                                       \n",
            "================================================================================================================================================================\n",
            "1    | 2512.23090v2    | 2025-12-28   | 2026-01-02   | cs.AI, cs.LG         | Benchmark Success, Clinical Failure: When Reinforcemen...   \n",
            "2    | 2501.19107v3    | 2025-01-31   | 2026-01-02   | cs.LG                | Brain network science modelling of sparse neural netwo...   \n",
            "3    | 2601.00770v1    | 2026-01-02   | 2026-01-02   | cs.CE, cs.AI         | LLM Agents for Combinatorial Efficient Frontiers: Inve...   \n",
            "4    | 2506.01495v5    | 2025-06-02   | 2026-01-02   | cs.CL                | C-VARC: A Large-Scale Chinese Value Rule Corpus for Va...   \n",
            "5    | 2601.00756v1    | 2026-01-02   | 2026-01-02   | cs.LG, cs.CL         | Memory Bank Compression for Continual Adaptation of La...   \n",
            "6    | 2601.00747v1    | 2026-01-02   | 2026-01-02   | cs.LG                | The Reasoning-Creativity Trade-off: Toward Creativity-...   \n",
            "7    | 2506.15131v2    | 2025-06-18   | 2026-01-02   | cs.CL, cs.AI         | Modeling the One-to-Many Property in Open-Domain Dialo...   \n",
            "8    | 2601.00742v1    | 2026-01-02   | 2026-01-02   | physics.comp-ph      | Materials Informatics: Emergence To Autonomous Discove...   \n",
            "9    | 2506.07675v3    | 2025-06-09   | 2026-01-02   | cs.DB, cs.AI         | QUITE: A Query Rewrite System Beyond Rules with LLM Ag...   \n",
            "10   | 2601.00736v1    | 2026-01-02   | 2026-01-02   | cs.CL, cs.AI         | Exploring the Performance of Large Language Models on ...   \n",
            "11   | 2502.05795v3    | 2025-02-09   | 2026-01-02   | cs.LG, cs.AI         | The Curse of Depth in Large Language Models                 \n",
            "12   | 2601.00730v1    | 2026-01-02   | 2026-01-02   | cs.CV                | Grading Handwritten Engineering Exams with Multimodal ...   \n",
            "13   | 2512.24637v2    | 2025-12-31   | 2026-01-02   | cs.OS                | Towards Fully-fledged GPU Multitasking via Proactive M...   \n",
            "14   | 2512.22905v2    | 2025-12-28   | 2026-01-02   | cs.CV                | JavisGPT: A Unified Multi-modal LLM for Sounding-Video...   \n",
            "15   | 2511.10684v4    | 2025-11-11   | 2026-01-02   | cs.CL, cs.CY         | SpiderGen: Towards Procedure Generation For Carbon Lif...   \n",
            "16   | 2601.00694v1    | 2026-01-02   | 2026-01-02   | cs.AI                | A Vision-and-Knowledge Enhanced Large Language Model f...   \n",
            "17   | 2512.24098v2    | 2025-12-30   | 2026-01-02   | cs.CL, cs.LG         | Training a Huggingface Model on AWS Sagemaker (Without...   \n",
            "18   | 2601.00685v1    | 2026-01-02   | 2026-01-02   | physics.med-ph       | Human-like AI-based Auto-Field-in-Field Whole-Brain Ra...   \n",
            "19   | 2412.11167v4    | 2024-12-15   | 2026-01-02   | cs.CL                | Cultural Palette: Pluralising Culture Alignment via Mu...   \n",
            "20   | 2601.00679v1    | 2026-01-02   | 2026-01-02   | cs.NE, cs.AI         | QSLM: A Performance- and Memory-aware Quantization Fra...   \n",
            "21   | 2512.09238v2    | 2025-12-10   | 2026-01-02   | cs.CL                | Training-free Context-adaptive Attention for Efficient...   \n",
            "22   | 2408.03541v4    | 2024-08-07   | 2026-01-02   | cs.CL, cs.AI         | EXAONE 3.0 7.8B Instruction Tuned Language Model            \n",
            "23   | 2601.00644v1    | 2026-01-02   | 2026-01-02   | cs.DC                | FlexSpec: Frozen Drafts Meet Evolving Targets in Edge-...   \n",
            "24   | 2601.00641v1    | 2026-01-02   | 2026-01-02   | cs.CL                | Probabilistic Guarantees for Reducing Contextual Hallu...   \n",
            "25   | 2601.00624v1    | 2026-01-02   | 2026-01-02   | cs.LG                | Do Chatbot LLMs Talk Too Much? The YapBench Benchmark       \n",
            "26   | 2507.22411v2    | 2025-07-30   | 2026-01-02   | cs.CL, cs.AI         | NeedleChain: Measuring Intact Context Comprehension Ca...   \n",
            "27   | 2601.00596v1    | 2026-01-02   | 2026-01-02   | cs.CL                | Beyond IVR: Benchmarking Customer Support LLM Agents f...   \n",
            "28   | 2601.00588v1    | 2026-01-02   | 2026-01-02   | cs.CL                | CSSBench: Evaluating the Safety of Lightweight LLMs ag...   \n",
            "29   | 2601.00583v1    | 2026-01-02   | 2026-01-02   | cs.LG, cs.AI         | HFedMoE: Resource-aware Heterogeneous Federated Learni...   \n",
            "30   | 2601.00575v1    | 2026-01-02   | 2026-01-02   | cs.CL                | InfoSynth: Information-Guided Benchmark Synthesis for ...   \n",
            "31   | 2601.00570v1    | 2026-01-02   | 2026-01-02   | cs.HC                | User Perceptions of an LLM-Based Chatbot for Cognitive...   \n",
            "32   | 2512.23959v2    | 2025-12-30   | 2026-01-02   | cs.CL, cs.AI         | Improving Multi-step RAG with Hypergraph-based Memory ...   \n",
            "33   | 2601.00567v1    | 2026-01-02   | 2026-01-02   | cs.IR, cs.AI         | Improving Scientific Document Retrieval with Academic ...   \n",
            "34   | 2601.00566v1    | 2026-01-02   | 2026-01-02   | cs.CR                | Low Rank Comes with Low Security: Gradient Assembly Po...   \n",
            "35   | 2512.24818v2    | 2025-12-31   | 2026-01-02   | cs.LG                | Unregularized Linear Convergence in Zero-Sum Game from...   \n",
            "36   | 2601.00559v1    | 2026-01-02   | 2026-01-02   | cs.CR, cs.AI         | Cracking IoT Security: Can LLMs Outsmart Static Analys...   \n",
            "37   | 2512.01661v2    | 2025-12-01   | 2026-01-02   | cs.CL, cs.AI         | Learning the Boundary of Solvability: Aligning LLMs to...   \n",
            "38   | 2601.00555v1    | 2026-01-02   | 2026-01-02   | cs.RO                | LLM-Based Agentic Exploration for Robot Navigation & M...   \n",
            "39   | 2503.02152v2    | 2025-03-04   | 2026-01-02   | cs.LG, cs.CL         | Tabby: A Language Model Architecture for Tabular and S...   \n",
            "40   | 2402.14746v6    | 2024-02-22   | 2026-01-02   | cs.CL, cs.LG         | Scaling Efficient LLMs                                      \n",
            "41   | 2509.21043v5    | 2025-09-25   | 2026-01-02   | cs.AI, cs.LG         | Combinatorial Creativity: A New Frontier in Generaliza...   \n",
            "42   | 2601.00516v1    | 2026-01-02   | 2026-01-02   | cs.LG, cs.AI         | Trajectory Guard -- A Lightweight, Sequence-Aware Mode...   \n",
            "43   | 2601.00510v1    | 2026-01-01   | 2026-01-01   | cs.IR, cs.CL         | A Chain-of-Thought Approach to Semantic Query Categori...   \n",
            "44   | 2601.00509v1    | 2026-01-01   | 2026-01-01   | cs.CR, cs.LG         | Improving LLM-Assisted Secure Code Generation through ...   \n",
            "45   | 2601.00501v1    | 2026-01-01   | 2026-01-01   | cs.CV                | CPPO: Contrastive Perception for Vision Language Polic...   \n",
            "46   | 2601.00497v1    | 2026-01-01   | 2026-01-01   | cs.SE                | STELLAR: A Search-Based Testing Framework for Large La...   \n",
            "47   | 2601.00482v1    | 2026-01-01   | 2026-01-01   | cs.SE, cs.AI         | Multi-Agent Coordinated Rename Refactoring                  \n",
            "48   | 2601.00481v1    | 2026-01-01   | 2026-01-01   | cs.NI, cs.AI         | MAESTRO: Multi-Agent Evaluation Suite for Testing, Rel...   \n",
            "49   | 2601.00469v1    | 2026-01-01   | 2026-01-01   | cs.SE                | DSL or Code? Evaluating the Quality of LLM-Generated A...   \n",
            "50   | 2508.02497v2    | 2025-08-04   | 2026-01-01   | cs.SE                | Towards Bridging Language Gaps in OSS with LLM-Driven ...   \n",
            "================================================================================================================================================================\n",
            "\n",
            "\n",
            "üì• DOWNLOAD OPTIONS:\n",
            "1. Download all PDFs only\n",
            "2. Download all PDFs + Extract to Markdown\n",
            "3. Select specific papers\n",
            "4. Quit\n",
            "\n",
            "üöÄ Downloading PDFs and extracting to Markdown...\n",
            "   ‚¨áÔ∏è Downloading: Benchmark Success, Clinical Failure: When Reinforc...\n",
            "   ‚úÖ PDF saved: papers_pdf\\2512.23090v2_Benchmark_Success_Clinical_Failure_When_Reinforcem.pdf\n",
            "   üÜî UUID: 8d2f27af-d657-4d40-93c7-59b114a219a5\n",
            "   üìù Citation: Armin Berger et al. (2025). Benchmark Success, Clinical Failure: When Reinforcem...\n",
            "   üìÑ Extracting sections from PDF...\n",
            "   ü§ñ Processing with Marker AI...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Recognizing Layout: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21/21 [01:51<00:00,  5.31s/it]\n",
            "Running OCR Error Detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.28it/s]\n",
            "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.28s/it]\n",
            "Recognizing Text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [05:51<00:00, 25.10s/it]\n",
            "Recognizing Text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:37<00:00,  9.38s/it]\n",
            "c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
            "  warnings.warn(\n",
            "Recognizing tables: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:06<00:00,  6.40s/it]\n",
            "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.31s/it]\n",
            "Recognizing Text: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167/167 [01:18<00:00,  2.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   üìù Extracted 57884 characters\n",
            "   ‚úÖ Markdown saved: papers_markdown\\8d2f27af_Benchmark_Success_Clinical_Failure_When_Reinforcem.md\n",
            "   ‚¨áÔ∏è Downloading: Brain network science modelling of sparse neural n...\n",
            "   ‚úÖ PDF saved: papers_pdf\\2501.19107v3_Brain_network_science_modelling_of_sparse_neural_n.pdf\n",
            "   üÜî UUID: e2b2a0f7-26ab-4625-9a93-931eca5fe1b5\n",
            "   üìù Citation: Yingtao Zhang et al. (2025). Brain network science modelling of sparse neural ne...\n",
            "   üìÑ Extracting sections from PDF...\n",
            "   ü§ñ Processing with Marker AI...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Recognizing Layout: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [05:34<00:00,  9.04s/it]\n",
            "Running OCR Error Detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:03<00:00,  2.64it/s]\n",
            "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.27s/it]\n",
            "Recognizing Text:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 147/161 [09:42<00:03,  3.57it/s] "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m pdf_path = paper_data[\u001b[33m'\u001b[39m\u001b[33mpdf_path\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   üìÑ Extracting sections from PDF...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m sections = \u001b[43mextract_sections_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sections:\n\u001b[32m     86\u001b[39m     md_path = create_selective_markdown(paper_uuid, paper_data, sections)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mextract_sections_from_pdf\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ü§ñ Processing with Marker AI...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Convert PDF to markdown using Marker AI\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m rendered = \u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m full_text, _, images = text_from_rendered(rendered)\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   üìù Extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(full_text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m characters\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\converters\\pdf.py:195\u001b[39m, in \u001b[36mPdfConverter.__call__\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath: \u001b[38;5;28mstr\u001b[39m | io.BytesIO):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filepath_to_str(filepath) \u001b[38;5;28;01mas\u001b[39;00m temp_path:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         document = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28mself\u001b[39m.page_count = \u001b[38;5;28mlen\u001b[39m(document.pages)\n\u001b[32m    197\u001b[39m         renderer = \u001b[38;5;28mself\u001b[39m.resolve_dependencies(\u001b[38;5;28mself\u001b[39m.renderer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\converters\\pdf.py:182\u001b[39m, in \u001b[36mPdfConverter.build_document\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    180\u001b[39m ocr_builder = \u001b[38;5;28mself\u001b[39m.resolve_dependencies(OcrBuilder)\n\u001b[32m    181\u001b[39m provider = provider_cls(filepath, \u001b[38;5;28mself\u001b[39m.config)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m document = \u001b[43mDocumentBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_builder\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m structure_builder_cls = \u001b[38;5;28mself\u001b[39m.resolve_dependencies(StructureBuilder)\n\u001b[32m    186\u001b[39m structure_builder_cls(document)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\document.py:36\u001b[39m, in \u001b[36mDocumentBuilder.__call__\u001b[39m\u001b[34m(self, provider, layout_builder, line_builder, ocr_builder)\u001b[39m\n\u001b[32m     34\u001b[39m line_builder(document, provider)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable_ocr:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mocr_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m document\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\ocr.py:87\u001b[39m, in \u001b[36mOcrBuilder.__call__\u001b[39m\u001b[34m(self, document, provider)\u001b[39m\n\u001b[32m     83\u001b[39m pages_to_ocr = [page \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m document.pages \u001b[38;5;28;01mif\u001b[39;00m page.text_extraction_method == \u001b[33m'\u001b[39m\u001b[33msurya\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     84\u001b[39m ocr_page_images, block_polygons, block_ids, block_original_texts = (\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_ocr_images_polygons_ids(document, pages_to_ocr, provider)\n\u001b[32m     86\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mocr_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpages_to_ocr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mocr_page_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_polygons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_original_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\ocr.py:178\u001b[39m, in \u001b[36mOcrBuilder.ocr_extraction\u001b[39m\u001b[34m(self, document, pages, images, block_polygons, block_ids, block_original_texts)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.recognition_model.disable_tqdm = \u001b[38;5;28mself\u001b[39m.disable_tqdm\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m recognition_results: List[OCRResult] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecognition_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mocr_task_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolygons\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_polygons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_original_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecognition_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_recognition_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisable_ocr_math\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_repeated_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdrop_repeated_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2148\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(recognition_results) == \u001b[38;5;28mlen\u001b[39m(images) == \u001b[38;5;28mlen\u001b[39m(pages) == \u001b[38;5;28mlen\u001b[39m(block_ids), (\n\u001b[32m    192\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMismatch in OCR lengths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(recognition_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(block_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    193\u001b[39m )\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document_page, page_recognition_result, page_block_ids, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    195\u001b[39m     pages, recognition_results, block_ids, images\n\u001b[32m    196\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\recognition\\__init__.py:431\u001b[39m, in \u001b[36mRecognitionPredictor.__call__\u001b[39m\u001b[34m(self, images, task_names, det_predictor, detection_batch_size, recognition_batch_size, highres_images, bboxes, polygons, input_text, sort_lines, math_mode, return_words, drop_repeated_text, max_sliding_window, max_tokens, filter_tag_list)\u001b[39m\n\u001b[32m    428\u001b[39m flat[\u001b[33m\"\u001b[39m\u001b[33mtask_names\u001b[39m\u001b[33m\"\u001b[39m] = [flat[\u001b[33m\"\u001b[39m\u001b[33mtask_names\u001b[39m\u001b[33m\"\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m predicted_tokens, batch_bboxes, scores, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfoundation_predictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprediction_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mslices\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecognition_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_repeated_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lookahead_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfoundation_predictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_output_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_sliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecognizing Text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Get text and bboxes in structured form\u001b[39;00m\n\u001b[32m    445\u001b[39m bbox_size = \u001b[38;5;28mself\u001b[39m.bbox_size\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\foundation\\__init__.py:827\u001b[39m, in \u001b[36mFoundationPredictor.prediction_loop\u001b[39m\u001b[34m(self, images, input_texts, task_names, batch_size, max_tokens, max_sliding_window, math_mode, drop_repeated_tokens, max_lookahead_tokens, top_k, tqdm_desc)\u001b[39m\n\u001b[32m    825\u001b[39m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m     updated_inputs, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lookahead_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_lookahead_tokens\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    830\u001b[39m     mark_step()\n\u001b[32m    832\u001b[39m     predicted_tokens_cpu = outputs.preds.cpu()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\foundation\\__init__.py:341\u001b[39m, in \u001b[36mFoundationPredictor.decode\u001b[39m\u001b[34m(self, current_inputs, max_lookahead_tokens)\u001b[39m\n\u001b[32m    337\u001b[39m cache_position = \u001b[38;5;28mself\u001b[39m.get_cache_position(\n\u001b[32m    338\u001b[39m     input_ids.shape[\u001b[32m1\u001b[39m], \u001b[38;5;28mself\u001b[39m.kv_cache.attention_mask, prefill=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    339\u001b[39m )\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m settings.INFERENCE_MODE():\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43membed_boxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m processed_output: ContinuousBatchOutput = \u001b[38;5;28mself\u001b[39m.process_outputs(\n\u001b[32m    356\u001b[39m     outputs, max_lookahead_tokens=max_lookahead_tokens\n\u001b[32m    357\u001b[39m )\n\u001b[32m    359\u001b[39m input_ids = processed_output.input_ids\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\__init__.py:468\u001b[39m, in \u001b[36mSuryaModel.forward\u001b[39m\u001b[34m(self, input_ids, image_embeddings, labels, image_tiles, grid_thw, inputs_embeds, attention_mask, position_ids, cache_position, past_key_values, output_hidden_states, output_attentions, use_cache, encoder_chunk_size, cache_idxs, num_valid_tokens, prefill, text_lengths, valid_batch_size, input_boxes, embed_boxes, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m causal_mask = \u001b[38;5;28mself\u001b[39m._update_causal_mask(\n\u001b[32m    460\u001b[39m     attention_mask,\n\u001b[32m    461\u001b[39m     inputs_embeds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m     output_attentions,\n\u001b[32m    465\u001b[39m )\n\u001b[32m    467\u001b[39m attention_mask = causal_mask\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logits_to_keep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\decoder\\__init__.py:504\u001b[39m, in \u001b[36mSuryaDecoderModel.forward\u001b[39m\u001b[34m(self, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, cache_idxs, num_valid_tokens, text_lengths, prefill, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\decoder\\__init__.py:302\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, cache_idxs, num_valid_tokens, text_lengths, prefill, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    299\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\decoder\\__init__.py:229\u001b[39m, in \u001b[36mQwen2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, cache_idxs, num_valid_tokens, text_lengths, prefill, **kwargs)\u001b[39m\n\u001b[32m    217\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[33;03mIMPORTANT:\u001b[39;00m\n\u001b[32m    219\u001b[39m \u001b[33;03mWe sometimes use a custom sliding window impl. during training\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m \u001b[33;03mSDPA ignores this completely, and is fully dependent on the 4D mask - (https://github.com/huggingface/transformers/blob/b9faf2f93085e3cf2c65184a69d1d9e502f95786/src/transformers/integrations/sdpa_attention.py#L23)\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m sliding_window = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# main diff with Llama\u001b[39;49;00m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    242\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:67\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mnum_key_value_groups\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_gqa_in_sdpa(attention_mask, key):\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m         key = \u001b[43mrepeat_kv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_key_value_groups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m         value = repeat_kv(value, module.num_key_value_groups)\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:27\u001b[39m, in \u001b[36mrepeat_kv\u001b[39m\u001b[34m(hidden_states, n_rep)\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n\u001b[32m     26\u001b[39m hidden_states = hidden_states[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_key_value_heads\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# ---------------------------------------------------------\n",
        "# MAIN EXECUTION\n",
        "# ---------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 80)\n",
        "    print(\"  üìö PAPER DOWNLOAD & EXTRACTION SYSTEM V6 (Marker AI)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"  üìÅ PDFs saved to: {PDF_FOLDER}/\")\n",
        "    print(f\"  üìù Markdown saved to: {MARKDOWN_FOLDER}/\")\n",
        "    print(f\"  üóÑÔ∏è  Metadata database: {METADATA_FILE}\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    if not MARKER_AVAILABLE:\n",
        "        print(\"\\n‚ö†Ô∏è  WARNING: Marker AI not available!\")\n",
        "        print(\"   PDF downloads will work, but extraction will be skipped.\")\n",
        "        print(\"   Install with: pip install marker-pdf --upgrade\")\n",
        "        continue_anyway = input(\"\\n   Continue anyway? (y/n): \").lower().strip()\n",
        "        if continue_anyway != 'y':\n",
        "            exit()\n",
        "    \n",
        "    # Initialize database\n",
        "    db = PaperDatabase()\n",
        "    \n",
        "    print(\"\\n1. arXiv\")\n",
        "    print(\"2. PubMed (PMC Open Access)\")\n",
        "\n",
        "    choice = input(\"Select Source (1 or 2): \").strip()\n",
        "    \n",
        "    if choice not in ['1', '2']:\n",
        "        print(\"‚ùå Invalid choice. Please select 1 or 2.\")\n",
        "        exit()\n",
        "    \n",
        "    topic = input(\"Enter search topic: \").strip()\n",
        "    \n",
        "    if not topic:\n",
        "        print(\"‚ùå Search topic cannot be empty.\")\n",
        "        exit()\n",
        "    \n",
        "    filters = None\n",
        "    if choice == '1':\n",
        "        use_filters = input(\"Use advanced filters? (y/n): \").lower().strip()\n",
        "        if use_filters == 'y':\n",
        "            filters = get_arxiv_filters()\n",
        "\n",
        "    print(f\"\\nüîç Searching for '{topic}' (Max {MAX_RESULTS})...\")\n",
        "\n",
        "    if choice == '1':\n",
        "        papers = get_arxiv(topic, filters)\n",
        "    else:\n",
        "        papers = get_pubmed(topic)\n",
        "\n",
        "    if not papers:\n",
        "        print(\"‚ùå No results found.\")\n",
        "        exit()\n",
        "\n",
        "    display_results(papers, choice)\n",
        "\n",
        "    print(\"\\nüì• DOWNLOAD OPTIONS:\")\n",
        "    print(\"1. Download all PDFs only\")\n",
        "    print(\"2. Download all PDFs + Extract to Markdown\")\n",
        "    print(\"3. Select specific papers\")\n",
        "    print(\"4. Quit\")\n",
        "    \n",
        "    mode = input(\"\\nYour choice (1-4): \").strip()\n",
        "\n",
        "    if mode == '1':\n",
        "        print(\"\\nüöÄ Downloading PDFs only...\")\n",
        "        for paper in papers:\n",
        "            download_and_save_pdf(paper, choice, db)\n",
        "            time.sleep(DOWNLOAD_DELAY)\n",
        "        print(f\"\\n‚úÖ Complete! Check {PDF_FOLDER}/ and {METADATA_FILE}\")\n",
        "        \n",
        "    elif mode == '2':\n",
        "        print(\"\\nüöÄ Downloading PDFs and extracting to Markdown...\")\n",
        "        for paper in papers:\n",
        "            success, paper_uuid = download_and_save_pdf(paper, choice, db)\n",
        "            if success and paper_uuid:\n",
        "                # Get metadata\n",
        "                paper_data = db.get_paper(paper_uuid)\n",
        "                pdf_path = paper_data['pdf_path']\n",
        "                \n",
        "                print(f\"   üìÑ Extracting sections from PDF...\")\n",
        "                sections = extract_sections_from_pdf(pdf_path)\n",
        "                \n",
        "                if sections:\n",
        "                    md_path = create_selective_markdown(paper_uuid, paper_data, sections)\n",
        "                    print(f\"   ‚úÖ Markdown saved: {md_path}\")\n",
        "                    \n",
        "                    # Update database with markdown path\n",
        "                    paper_data['markdown_path'] = md_path\n",
        "                    db.save()\n",
        "                    \n",
        "            time.sleep(DOWNLOAD_DELAY)\n",
        "        print(f\"\\n‚úÖ Complete! Check {PDF_FOLDER}/, {MARKDOWN_FOLDER}/ and {METADATA_FILE}\")\n",
        "        \n",
        "    elif mode == '3':\n",
        "        indices = input(\"Enter paper numbers (comma-separated, e.g., 1,3,5): \").strip()\n",
        "        try:\n",
        "            selected = [int(x.strip()) - 1 for x in indices.split(',')]\n",
        "            extract = input(\"Extract to markdown? (y/n): \").lower().strip() == 'y'\n",
        "            \n",
        "            for idx in selected:\n",
        "                if 0 <= idx < len(papers):\n",
        "                    success, paper_uuid = download_and_save_pdf(papers[idx], choice, db)\n",
        "                    \n",
        "                    if success and extract and paper_uuid:\n",
        "                        paper_data = db.get_paper(paper_uuid)\n",
        "                        pdf_path = paper_data['pdf_path']\n",
        "                        \n",
        "                        print(f\"   üìÑ Extracting sections...\")\n",
        "                        sections = extract_sections_from_pdf(pdf_path)\n",
        "                        \n",
        "                        if sections:\n",
        "                            md_path = create_selective_markdown(paper_uuid, paper_data, sections)\n",
        "                            print(f\"   ‚úÖ Markdown saved: {md_path}\")\n",
        "                            paper_data['markdown_path'] = md_path\n",
        "                            db.save()\n",
        "                    \n",
        "                    time.sleep(DOWNLOAD_DELAY)\n",
        "        except:\n",
        "            print(\"Invalid input\")\n",
        "            \n",
        "    else:\n",
        "        print(\"Exiting...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"üìä Total papers in database: {len(db.papers)}\")\n",
        "    print(\"=\" * 80)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "article",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
