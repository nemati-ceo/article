{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8JE3aBvTHHL",
        "outputId": "36138a9d-315c-4c2e-9673-d2133c531ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting marker-pdf\n",
            "  Downloading marker_pdf-1.10.1-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting pymed\n",
            "  Downloading pymed-0.8.9-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting Pillow<11.0.0,>=10.1.0 (from marker-pdf)\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting anthropic<0.47.0,>=0.46.0 (from marker-pdf)\n",
            "  Downloading anthropic-0.46.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (8.3.1)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from marker-pdf)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting ftfy<7.0.0,>=6.1.1 (from marker-pdf)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (1.55.0)\n",
            "Collecting markdown2<3.0.0,>=2.5.2 (from marker-pdf)\n",
            "  Downloading markdown2-2.5.4-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting markdownify<2.0.0,>=1.1.0 (from marker-pdf)\n",
            "  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting openai<2.0.0,>=1.65.2 (from marker-pdf)\n",
            "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting pdftext<0.7.0,>=0.6.3 (from marker-pdf)\n",
            "  Downloading pdftext-0.6.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting pre-commit<5.0.0,>=4.2.0 (from marker-pdf)\n",
            "  Downloading pre_commit-4.5.1-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (2.12.3)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (2.12.0)\n",
            "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (1.2.1)\n",
            "Collecting rapidfuzz<4.0.0,>=3.8.1 (from marker-pdf)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting regex<2025.0.0,>=2024.4.28 (from marker-pdf)\n",
            "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (1.6.1)\n",
            "Collecting surya-ocr<0.18.0,>=0.17.0 (from marker-pdf)\n",
            "  Downloading surya_ocr-0.17.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (4.67.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.45.2 in /usr/local/lib/python3.12/dist-packages (from marker-pdf) (4.57.3)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic<0.47.0,>=0.46.0->marker-pdf) (1.3.1)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy<7.0.0,>=6.1.1->marker-pdf) (0.2.14)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (2.43.0)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->marker-pdf) (9.1.2)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->marker-pdf) (15.0.1)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9 in /usr/local/lib/python3.12/dist-packages (from markdownify<2.0.0,>=1.1.0->marker-pdf) (4.13.5)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.12/dist-packages (from markdownify<2.0.0,>=1.1.0->marker-pdf) (1.17.0)\n",
            "Collecting pypdfium2==4.30.0 (from pdftext<0.7.0,>=0.6.3->marker-pdf)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cfgv>=2.0.0 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
            "  Downloading cfgv-3.5.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
            "  Downloading identify-2.6.15-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
            "  Downloading nodeenv-1.10.0-py2.py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from pre-commit<5.0.0,>=4.2.0->marker-pdf) (6.0.3)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
            "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.4.2->marker-pdf) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.6.1->marker-pdf) (3.6.0)\n",
            "Requirement already satisfied: einops<0.9.0,>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from surya-ocr<0.18.0,>=0.17.0->marker-pdf) (0.8.1)\n",
            "Collecting opencv-python-headless==4.11.0.86 (from surya-ocr<0.18.0,>=0.17.0->marker-pdf)\n",
            "  Downloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: platformdirs<5.0.0,>=4.3.6 in /usr/local/lib/python3.12/dist-packages (from surya-ocr<0.18.0,>=0.17.0->marker-pdf) (4.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (25.0)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.45.2->marker-pdf) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.9->markdownify<2.0.0,>=1.1.0->marker-pdf) (2.8)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic<0.47.0,>=0.46.0->marker-pdf) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers<5.0.0,>=4.45.2->marker-pdf) (1.2.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit<5.0.0,>=4.2.0->marker-pdf)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-auth[requests]<3.0.0,>=2.14.1->google-genai<2.0.0,>=1.0.0->marker-pdf) (0.6.1)\n",
            "Downloading marker_pdf-1.10.1-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.9/188.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arxiv-2.3.1-py3-none-any.whl (11 kB)\n",
            "Downloading pymed-0.8.9-py3-none-any.whl (9.6 kB)\n",
            "Downloading anthropic-0.46.0-py3-none-any.whl (223 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m223.2/223.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.4-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdownify-1.2.2-py3-none-any.whl (15 kB)\n",
            "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdftext-0.6.3-py3-none-any.whl (23 kB)\n",
            "Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.5.1-py2.py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading surya_ocr-0.17.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python_headless-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.5.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Downloading identify-2.6.15-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.10.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m141.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=7b8e9d0b0f24d8c6e33fff25c39d1f8f1a774d60a3890b59ad3bddbf90fdac3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, filetype, distlib, virtualenv, regex, rapidfuzz, pypdfium2, Pillow, opencv-python-headless, nodeenv, markdown2, identify, ftfy, feedparser, cfgv, pymed, pre-commit, markdownify, arxiv, openai, anthropic, pdftext, surya-ocr, marker-pdf\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2025.11.3\n",
            "    Uninstalling regex-2025.11.3:\n",
            "      Successfully uninstalled regex-2025.11.3\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.12.0.88\n",
            "    Uninstalling opencv-python-headless-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-headless-4.12.0.88\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 2.12.0\n",
            "    Uninstalling openai-2.12.0:\n",
            "      Successfully uninstalled openai-2.12.0\n",
            "Successfully installed Pillow-10.4.0 anthropic-0.46.0 arxiv-2.3.1 cfgv-3.5.0 distlib-0.4.0 feedparser-6.0.12 filetype-1.2.0 ftfy-6.3.1 identify-2.6.15 markdown2-2.5.4 markdownify-1.2.2 marker-pdf-1.10.1 nodeenv-1.10.0 openai-1.109.1 opencv-python-headless-4.11.0.86 pdftext-0.6.3 pre-commit-4.5.1 pymed-0.8.9 pypdfium2-4.30.0 rapidfuzz-3.14.3 regex-2024.11.6 sgmllib3k-1.0.0 surya-ocr-0.17.0 virtualenv-20.35.4\n"
          ]
        }
      ],
      "source": [
        "!pip install marker-pdf arxiv pymed requests torch biopython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6yHE3a0XK_R",
        "outputId": "7d8b49d2-3301-41ea-d92f-78cfcd327c55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â³ Loading Marker AI models...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\llmserver\\AppData\\Local\\Temp\\ipykernel_67708\\2169762549.py\", line 8, in <module>\n",
            "    from marker.converters.pdf import PdfConverter\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\converters\\pdf.py\", line 17, in <module>\n",
            "    from marker.builders.document import DocumentBuilder\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\document.py\", line 4, in <module>\n",
            "    from marker.builders.layout import LayoutBuilder\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\layout.py\", line 3, in <module>\n",
            "    from surya.layout import LayoutPredictor\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\layout\\__init__.py\", line 8, in <module>\n",
            "    from surya.foundation import FoundationPredictor, TaskNames\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\foundation\\__init__.py\", line 15, in <module>\n",
            "    from surya.common.surya import SuryaModelOutput\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\__init__.py\", line 8, in <module>\n",
            "    from transformers.modeling_outputs import CausalLMOutputWithPast\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\__init__.py\", line 27, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
            "    from .utils.versions import require_version, require_version_core\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\utils\\__init__.py\", line 24, in <module>\n",
            "    from .auto_docstring import (\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py\", line 30, in <module>\n",
            "    from .generic import ModelOutput\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 53, in <module>\n",
            "    from ..model_debugging_utils import model_addition_debugger_context\n",
            "  File \"c:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\model_debugging_utils.py\", line 30, in <module>\n",
            "    from safetensors.torch import save_file\n",
            "  File \"C:\\Users\\llmserver\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\__init__.py\", line 2, in <module>\n",
            "    from ._safetensors_rust import (  # noqa: F401\n",
            "ModuleNotFoundError: No module named 'safetensors._safetensors_rust'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ ImportError while importing Marker.\n",
            "   Python: c:\\Users\\llmserver\\miniconda3\\envs\\article\\python.exe\n",
            "   Error : No module named 'safetensors._safetensors_rust'\n",
            "\n",
            "âœ… Fix:\n",
            "   1) Activate your env:  conda activate article\n",
            "   2) Install correct pkg: python -m pip install -U marker-pdf\n",
            "\n",
            "ğŸ” Full traceback:\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'safetensors._safetensors_rust'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâ³ Loading Marker AI models...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconverters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfConverter\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_model_dict\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m text_from_rendered\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\converters\\pdf.py:17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllm_table_merge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMTableMergeProcessor\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mproviders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m provider_from_filepath\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocumentBuilder\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutBuilder\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LineBuilder\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\document.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Annotated\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseBuilder\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutBuilder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LineBuilder\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mocr\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OcrBuilder\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\layout.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Annotated, List\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutPredictor\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutResult, LayoutBox\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmarker\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseBuilder\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\layout\\__init__.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mschema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LayoutBox, LayoutResult\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msettings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m settings\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfoundation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FoundationPredictor, TaskNames\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfoundation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prediction_to_polygon_batch\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minput\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_if_not_rgb\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\foundation\\__init__.py:15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msurya\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SuryaModelOutput\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mxla\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mark_step\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msurya\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpredictor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BasePredictor\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\__init__.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CausalLMOutputWithPast\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcache_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cache\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_attn_mask_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionMaskConverter\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\__init__.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     OptionalDependencyNotAvailable,\n\u001b[32m     30\u001b[39m     _LazyModule,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     is_pretty_midi_available,\n\u001b[32m     37\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\dependency_versions_check.py:16\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdependency_versions_table\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[32m     25\u001b[39m pkgs_to_check_at_runtime = [\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtqdm\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpyyaml\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\utils\\__init__.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto_docstring\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     ClassAttrs,\n\u001b[32m     26\u001b[39m     ClassDocstring,\n\u001b[32m     27\u001b[39m     ImageProcessorArgs,\n\u001b[32m     28\u001b[39m     ModelArgs,\n\u001b[32m     29\u001b[39m     ModelOutputArgs,\n\u001b[32m     30\u001b[39m     auto_class_docstring,\n\u001b[32m     31\u001b[39m     auto_docstring,\n\u001b[32m     32\u001b[39m     get_args_doc_from_source,\n\u001b[32m     33\u001b[39m     parse_docstring,\n\u001b[32m     34\u001b[39m     set_min_indent,\n\u001b[32m     35\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackbone_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BackboneConfigMixin, BackboneMixin\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_template_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringParsingException, TypeHintParsingException, get_json_schema\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\utils\\auto_docstring.py:30\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mregex\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdoc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODELS_TO_PIPELINE,\n\u001b[32m     26\u001b[39m     PIPELINE_TASKS_TO_SAMPLE_DOCSTRINGS,\n\u001b[32m     27\u001b[39m     PT_SAMPLE_DOCSTRINGS,\n\u001b[32m     28\u001b[39m     _prepare_output_docstrings,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelOutput\n\u001b[32m     33\u001b[39m PATH_TO_TRANSFORMERS = Path(\u001b[33m\"\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m\"\u001b[39m).resolve() / \u001b[33m\"\u001b[39m\u001b[33mtransformers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     36\u001b[39m AUTODOC_FILES = [\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mconfiguration_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodeling_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeature_extractor_*.py\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     44\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\utils\\generic.py:53\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# required for @can_return_tuple decorator to work with torchdynamo\u001b[39;00m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_debugging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_addition_debugger_context\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# vendored from distutils.util\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstrtobool\u001b[39m(val):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\transformers\\model_debugging_utils.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msafetensors\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_file\n\u001b[32m     32\u001b[39m     _torch_distributed_available = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# Note to code inspectors: this toolbox is intended for people who add models to `transformers`.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\safetensors\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Re-export this\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_safetensors_rust\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      3\u001b[39m     SafetensorError,\n\u001b[32m      4\u001b[39m     __version__,\n\u001b[32m      5\u001b[39m     deserialize,\n\u001b[32m      6\u001b[39m     safe_open,\n\u001b[32m      7\u001b[39m     serialize,\n\u001b[32m      8\u001b[39m     serialize_file,\n\u001b[32m      9\u001b[39m )\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'safetensors._safetensors_rust'"
          ]
        }
      ],
      "source": [
        "# --- MARKER AI SETUP ---\n",
        "import sys\n",
        "import traceback\n",
        "\n",
        "print(\"â³ Loading Marker AI models...\")\n",
        "\n",
        "try:\n",
        "    from marker.converters.pdf import PdfConverter\n",
        "    from marker.models import create_model_dict\n",
        "    from marker.output import text_from_rendered\n",
        "\n",
        "    converter = PdfConverter(\n",
        "        artifact_dict=create_model_dict(),\n",
        "    )\n",
        "    print(\"âœ… Marker models loaded.\")\n",
        "\n",
        "except ImportError as e:\n",
        "    print(\"âŒ ImportError while importing Marker.\")\n",
        "    print(f\"   Python: {sys.executable}\")\n",
        "    print(f\"   Error : {e}\")\n",
        "    print(\"\\nâœ… Fix:\")\n",
        "    print(\"   1) Activate your env:  conda activate article\")\n",
        "    print(\"   2) Install correct pkg: python -m pip install -U marker-pdf\")\n",
        "    print(\"\\nğŸ” Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "    raise  # fail loudly so you see the real reason\n",
        "\n",
        "except Exception:\n",
        "    print(\"âŒ Model Load Error (non-import).\")\n",
        "    print(f\"   Python: {sys.executable}\")\n",
        "    print(\"\\nğŸ” Full traceback:\")\n",
        "    traceback.print_exc()\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl5-ZsniTJrx",
        "outputId": "2ff17839-23b3-49ba-96f7-8c877a8d698b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- MARKER AI FETCHER V4 ---\n",
            "1. arXiv\n",
            "2. PubMed (PMC Open Access)\n",
            "Select Source (1 or 2): 1\n",
            "Enter search topic: llm\n",
            "\n",
            "ğŸ” Searching for 'llm' (Max 50)...\n",
            "\n",
            "========================================================================================================================\n",
            "#    | Year   | Title                                              | Link\n",
            "========================================================================================================================\n",
            "1    | 2023   | RETA-LLM: A Retrieval-Augmented Large Langua...    | http://arxiv.org/abs/2306.05212v1\n",
            "2    | 2024   | FBI-LLM: Scaling Up Fully Binarized LLMs fro...    | http://arxiv.org/abs/2407.07093v1\n",
            "3    | 2024   | Systematic Evaluation of LLM-as-a-Judge in L...    | http://arxiv.org/abs/2408.13006v2\n",
            "4    | 2024   | Demystifying AI Platform Design for Distribu...    | http://arxiv.org/abs/2406.01698v3\n",
            "5    | 2025   | Can LLMs Lie? Investigation beyond Hallucina...    | http://arxiv.org/abs/2509.03518v1\n",
            "6    | 2024   | Open-LLM-Leaderboard: From Multi-choice to O...    | http://arxiv.org/abs/2406.07545v1\n",
            "7    | 2024   | Small LLMs Are Weak Tool Learners: A Multi-L...    | http://arxiv.org/abs/2401.07324v3\n",
            "8    | 2025   | A Survey of LLM $\\times$ DATA                      | http://arxiv.org/abs/2505.18458v3\n",
            "9    | 2025   | Enhancing Jailbreak Attacks on LLMs via Pers...    | http://arxiv.org/abs/2507.22171v2\n",
            "10   | 2024   | Any-Precision LLM: Low-Cost Deployment of Mu...    | http://arxiv.org/abs/2402.10517v4\n",
            "11   | 2025   | Harnessing Multiple Large Language Models: A...    | http://arxiv.org/abs/2502.18036v5\n",
            "12   | 2025   | RoleRAG: Enhancing LLM Role-Playing via Grap...    | http://arxiv.org/abs/2505.18541v1\n",
            "13   | 2024   | ARB-LLM: Alternating Refined Binarizations f...    | http://arxiv.org/abs/2410.03129v2\n",
            "14   | 2024   | A Survey on LLM-as-a-Judge                         | http://arxiv.org/abs/2411.15594v6\n",
            "15   | 2024   | Strategist: Self-improvement of LLM Decision...    | http://arxiv.org/abs/2408.10635v3\n",
            "16   | 2024   | ODA: Observation-Driven Agent for integratin...    | http://arxiv.org/abs/2404.07677v2\n",
            "17   | 2025   | Interactive Learning for LLM Reasoning             | http://arxiv.org/abs/2509.26306v3\n",
            "18   | 2025   | ELIS: Efficient LLM Iterative Scheduling Sys...    | http://arxiv.org/abs/2505.09142v1\n",
            "19   | 2024   | VerilogReader: LLM-Aided Hardware Test Gener...    | http://arxiv.org/abs/2406.04373v1\n",
            "20   | 2024   | C2HLSC: Can LLMs Bridge the Software-to-Hard...    | http://arxiv.org/abs/2406.09233v1\n",
            "21   | 2025   | Capability Instruction Tuning: A New Paradig...    | http://arxiv.org/abs/2502.17282v1\n",
            "22   | 2024   | Bridging Speech and Text: Enhancing ASR with...    | http://arxiv.org/abs/2409.16005v1\n",
            "23   | 2023   | FederatedScope-LLM: A Comprehensive Package ...    | http://arxiv.org/abs/2309.00363v1\n",
            "24   | 2024   | CATP-LLM: Empowering Large Language Models f...    | http://arxiv.org/abs/2411.16313v3\n",
            "25   | 2024   | SVD-LLM: Truncation-aware Singular Value Dec...    | http://arxiv.org/abs/2403.07378v5\n",
            "26   | 2025   | Rethinking Human Preference Evaluation of LL...    | http://arxiv.org/abs/2509.11026v1\n",
            "27   | 2025   | A Plan Reuse Mechanism for LLM-Driven Agent        | http://arxiv.org/abs/2512.21309v2\n",
            "28   | 2024   | When LLMs Play the Telephone Game: Cultural ...    | http://arxiv.org/abs/2407.04503v3\n",
            "29   | 2023   | AutoDroid: LLM-powered Task Automation in An...    | http://arxiv.org/abs/2308.15272v4\n",
            "30   | 2025   | Brain-Grounded Axes for Reading and Steering...    | http://arxiv.org/abs/2512.19399v1\n",
            "31   | 2025   | Self-Evaluating LLMs for Multi-Step Tasks: S...    | http://arxiv.org/abs/2511.07364v1\n",
            "32   | 2025   | Nuclear Deployed: Analyzing Catastrophic Ris...    | http://arxiv.org/abs/2502.11355v3\n",
            "33   | 2025   | Improving LLM-Powered EDA Assistants with RAFT     | http://arxiv.org/abs/2506.06500v1\n",
            "34   | 2024   | Self-Control of LLM Behaviors by Compressing...    | http://arxiv.org/abs/2406.02721v3\n",
            "35   | 2025   | SVD-LLM V2: Optimizing Singular Value Trunca...    | http://arxiv.org/abs/2503.12340v1\n",
            "36   | 2024   | FinCon: A Synthesized LLM Multi-Agent System...    | http://arxiv.org/abs/2407.06567v3\n",
            "37   | 2024   | Multi-Turn Human-LLM Interaction Through the...    | http://arxiv.org/abs/2410.20600v4\n",
            "38   | 2025   | RTL++: Graph-enhanced LLM for RTL Code Gener...    | http://arxiv.org/abs/2505.13479v1\n",
            "39   | 2025   | DINGO: Constrained Inference for Diffusion LLMs    | http://arxiv.org/abs/2505.23061v1\n",
            "40   | 2024   | Can Unconfident LLM Annotations Be Used for ...    | http://arxiv.org/abs/2408.15204v2\n",
            "41   | 2024   | Bias and Unfairness in Information Retrieval...    | http://arxiv.org/abs/2404.11457v2\n",
            "42   | 2025   | Improved LLM Agents for Financial Document Q...    | http://arxiv.org/abs/2506.08726v2\n",
            "43   | 2024   | LLM-Optic: Unveiling the Capabilities of Lar...    | http://arxiv.org/abs/2405.17104v2\n",
            "44   | 2024   | Beyond Binary: Towards Fine-Grained LLM-Gene...    | http://arxiv.org/abs/2410.14259v2\n",
            "45   | 2025   | Augmenting Dialog with Think-Aloud Utterance...    | http://arxiv.org/abs/2510.09158v2\n",
            "46   | 2025   | Collaborative Device-Cloud LLM Inference thr...    | http://arxiv.org/abs/2509.24050v1\n",
            "47   | 2023   | LLM-Prop: Predicting Physical And Electronic...    | http://arxiv.org/abs/2310.14029v1\n",
            "48   | 2025   | LLMs vs. Traditional Sentiment Tools in Psyc...    | http://arxiv.org/abs/2511.07641v1\n",
            "49   | 2025   | Combining TSL and LLM to Automate REST API T...    | http://arxiv.org/abs/2509.05540v1\n",
            "50   | 2025   | LLM+MAP: Bimanual Robot Task Planning using ...    | http://arxiv.org/abs/2503.17309v1\n",
            "========================================================================================================================\n",
            "\n",
            "Found 50 papers. Download ALL? (y/n): 1\n",
            "Enter number to download: 1\n",
            "   â¬‡ï¸ Fetching PDF: RETA-LLM: A Retrieval-Augmented Large La...\n",
            "   ğŸ§  Converting to Markdown (AI)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Recognizing Layout: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:05<00:00,  1.02s/it]\n",
            "Running OCR Error Detection: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 21.84it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n",
            "Detecting bboxes: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   âœ… Saved: papers_markdown/RETALLM A RetrievalAugmented Large Language Model .md\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import arxiv\n",
        "from Bio import Entrez\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "MAX_RESULTS = 50\n",
        "DOWNLOAD_DELAY = 2\n",
        "NCBI_API_KEY = \"162cefdacd4448a08831092c05eab6e73a09\"\n",
        "NCBI_EMAIL = \"shakeri163@gmail.com\"  # Update with your email\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# CONVERSION LOGIC\n",
        "# ---------------------------------------------------------\n",
        "def convert_to_markdown(pdf_path, title, metadata):\n",
        "    try:\n",
        "        rendered = converter(pdf_path)\n",
        "        full_text, _, images = text_from_rendered(rendered)\n",
        "\n",
        "        header = (\n",
        "            f\"# {title}\\n\\n\"\n",
        "            f\"**Source:** {metadata['source']}\\n\"\n",
        "            f\"**Date:** {metadata['date']}\\n\"\n",
        "            f\"**Authors:** {metadata['authors']}\\n\"\n",
        "            f\"**Link:** {metadata['url']}\\n\\n\"\n",
        "            f\"## Abstract\\n{metadata['abstract']}\\n\"\n",
        "            f\"---\\n\\n\"\n",
        "        )\n",
        "        return header + full_text\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ Marker conversion failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_markdown(title, markdown_content):\n",
        "    folder = \"papers_markdown\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    safe_title = \"\".join([c for c in title if c.isalpha() or c.isdigit() or c==' ']).strip()\n",
        "    safe_title = safe_title[:50]\n",
        "    md_path = os.path.join(folder, f\"{safe_title}.md\")\n",
        "\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(markdown_content)\n",
        "    return md_path\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# DOWNLOAD HELPER\n",
        "# ---------------------------------------------------------\n",
        "def process_paper(title, download_func, metadata):\n",
        "    print(f\"   â¬‡ï¸ Fetching PDF: {title[:40]}...\")\n",
        "    pdf_path = \"temp_processing.pdf\"\n",
        "\n",
        "    if not download_func(pdf_path):\n",
        "        print(\"   âŒ Download failed (Paywall or Network Error).\")\n",
        "        return False\n",
        "\n",
        "    print(f\"   ğŸ§  Converting to Markdown (AI)...\")\n",
        "    md_content = convert_to_markdown(pdf_path, title, metadata)\n",
        "\n",
        "    if md_content:\n",
        "        path = save_markdown(title, md_content)\n",
        "        print(f\"   âœ… Saved: {path}\")\n",
        "\n",
        "    if os.path.exists(pdf_path):\n",
        "        os.remove(pdf_path)\n",
        "    return True\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# ARXIV HANDLERS\n",
        "# ---------------------------------------------------------\n",
        "def get_arxiv(query):\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(query=query, max_results=MAX_RESULTS, sort_by=arxiv.SortCriterion.Relevance)\n",
        "    return list(client.results(search))\n",
        "\n",
        "def download_arxiv_wrapper(paper):\n",
        "    def downloader(path):\n",
        "        try:\n",
        "            paper.download_pdf(filename=path)\n",
        "            return True\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    meta = {\n",
        "        'source': 'arXiv',\n",
        "        'url': paper.entry_id,\n",
        "        'date': paper.published.year,\n",
        "        'authors': ', '.join(a.name for a in paper.authors),\n",
        "        'abstract': paper.summary\n",
        "    }\n",
        "    return process_paper(paper.title, downloader, meta)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# PUBMED HANDLERS (Biopython Entrez)\n",
        "# ---------------------------------------------------------\n",
        "def get_pubmed(query):\n",
        "    Entrez.email = NCBI_EMAIL\n",
        "    Entrez.api_key = NCBI_API_KEY\n",
        "\n",
        "    try:\n",
        "        # Search PMC for open access articles\n",
        "        handle = Entrez.esearch(db=\"pmc\", term=query, retmax=MAX_RESULTS)\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        ids = record.get(\"IdList\", [])\n",
        "        if not ids:\n",
        "            return []\n",
        "\n",
        "        # Fetch summaries for each article\n",
        "        handle = Entrez.esummary(db=\"pmc\", id=\",\".join(ids))\n",
        "        summaries = Entrez.read(handle)\n",
        "        handle.close()\n",
        "\n",
        "        # Build paper objects with consistent structure\n",
        "        papers = []\n",
        "        for summary in summaries:\n",
        "            papers.append({\n",
        "                'pmc_id': f\"PMC{summary.get('Id', '')}\",\n",
        "                'title': summary.get('Title', 'No title'),\n",
        "                'authors': summary.get('AuthorList', []),\n",
        "                'date': summary.get('PubDate', 'N/A'),\n",
        "                'source': summary.get('Source', ''),\n",
        "            })\n",
        "        return papers\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ PubMed API Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def get_pubmed_abstract(pmc_id):\n",
        "    \"\"\"Fetch abstract for a single PMC article.\"\"\"\n",
        "    try:\n",
        "        handle = Entrez.efetch(db=\"pmc\", id=pmc_id.replace(\"PMC\", \"\"), rettype=\"xml\")\n",
        "        content = handle.read()\n",
        "        handle.close()\n",
        "        # Basic extraction - abstract is between <abstract> tags\n",
        "        if b'<abstract>' in content:\n",
        "            start = content.find(b'<abstract>') + 10\n",
        "            end = content.find(b'</abstract>')\n",
        "            abstract = content[start:end].decode('utf-8', errors='ignore')\n",
        "            # Strip XML tags\n",
        "            import re\n",
        "            abstract = re.sub(r'<[^>]+>', '', abstract).strip()\n",
        "            return abstract[:500] + \"...\" if len(abstract) > 500 else abstract\n",
        "    except:\n",
        "        pass\n",
        "    return \"Abstract not available\"\n",
        "\n",
        "def download_pubmed_wrapper(paper):\n",
        "    pmc_id = paper['pmc_id']\n",
        "\n",
        "    def downloader(path):\n",
        "        import re\n",
        "        import tarfile\n",
        "        import io\n",
        "\n",
        "        # Step 1: Get package URL from OA service\n",
        "        oa_url = f\"https://www.ncbi.nlm.nih.gov/pmc/utils/oa/oa.fcgi?id={pmc_id}\"\n",
        "\n",
        "        try:\n",
        "            r = requests.get(oa_url, timeout=30)\n",
        "            if r.status_code != 200:\n",
        "                print(f\"      OA service failed: {r.status_code}\")\n",
        "                return False\n",
        "\n",
        "            # Find tar.gz link\n",
        "            tgz_match = re.search(r'href=\"(ftp://[^\"]+\\.tar\\.gz)\"', r.text)\n",
        "            if not tgz_match:\n",
        "                print(f\"      No tar.gz package found\")\n",
        "                return False\n",
        "\n",
        "            tgz_url = tgz_match.group(1)\n",
        "            # Convert FTP to HTTPS\n",
        "            tgz_url = tgz_url.replace(\"ftp://ftp.ncbi.nlm.nih.gov/\", \"https://ftp.ncbi.nlm.nih.gov/\")\n",
        "            print(f\"      Downloading package: {tgz_url}\")\n",
        "\n",
        "            # Download tar.gz\n",
        "            r2 = requests.get(tgz_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=120)\n",
        "            if r2.status_code != 200:\n",
        "                print(f\"      Package download failed: {r2.status_code}\")\n",
        "                return False\n",
        "\n",
        "            # Extract PDF from tar.gz\n",
        "            print(f\"      Extracting PDF from archive...\")\n",
        "            tar_bytes = io.BytesIO(r2.content)\n",
        "            with tarfile.open(fileobj=tar_bytes, mode='r:gz') as tar:\n",
        "                for member in tar.getmembers():\n",
        "                    if member.name.endswith('.pdf'):\n",
        "                        print(f\"      Found: {member.name}\")\n",
        "                        pdf_file = tar.extractfile(member)\n",
        "                        if pdf_file:\n",
        "                            with open(path, 'wb') as f:\n",
        "                                f.write(pdf_file.read())\n",
        "                            return True\n",
        "\n",
        "            print(f\"      No PDF found in archive\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      Error: {type(e).__name__}: {e}\")\n",
        "            return False\n",
        "\n",
        "    meta = {\n",
        "        'source': 'PubMed Central',\n",
        "        'url': f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{pmc_id}/\",\n",
        "        'date': paper['date'],\n",
        "        'authors': ', '.join(paper['authors'][:3]) if paper['authors'] else 'Unknown',\n",
        "        'abstract': get_pubmed_abstract(pmc_id)\n",
        "    }\n",
        "    return process_paper(paper['title'], downloader, meta)\n",
        "# ---------------------------------------------------------\n",
        "# DISPLAY HELPERS\n",
        "# ---------------------------------------------------------\n",
        "def display_results(papers, source_type):\n",
        "    print(\"\\n\" + \"=\" * 120)\n",
        "    print(f\"{'#':<4} | {'Year':<6} | {'Title':<50} | {'Link'}\")\n",
        "    print(\"=\" * 120)\n",
        "\n",
        "    for i, p in enumerate(papers):\n",
        "        if source_type == '1':  # arXiv\n",
        "            year = str(p.published.year)\n",
        "            link = p.entry_id\n",
        "            title = p.title.replace('\\n', ' ')\n",
        "        else:  # PubMed\n",
        "            year = str(p['date'])[:4] if p['date'] else \"N/A\"\n",
        "            link = f\"https://www.ncbi.nlm.nih.gov/pmc/articles/{p['pmc_id']}/\"\n",
        "            title = p['title'].replace('\\n', ' ')\n",
        "\n",
        "        if len(title) > 47:\n",
        "            title = title[:44] + \"...\"\n",
        "\n",
        "        print(f\"{i+1:<4} | {year:<6} | {title:<50} | {link}\")\n",
        "\n",
        "    print(\"=\" * 120 + \"\\n\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# MAIN EXECUTION\n",
        "# ---------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- MARKER AI FETCHER V4 ---\")\n",
        "    print(\"1. arXiv\")\n",
        "    print(\"2. PubMed (PMC Open Access)\")\n",
        "\n",
        "    choice = input(\"Select Source (1 or 2): \")\n",
        "    topic = input(\"Enter search topic: \")\n",
        "\n",
        "    print(f\"\\nğŸ” Searching for '{topic}' (Max {MAX_RESULTS})...\")\n",
        "\n",
        "    if choice == '1':\n",
        "        papers = get_arxiv(topic)\n",
        "    elif choice == '2':\n",
        "        papers = get_pubmed(topic)\n",
        "    else:\n",
        "        print(\"Invalid choice.\")\n",
        "        exit()\n",
        "\n",
        "    if not papers:\n",
        "        print(\"âŒ No results found.\")\n",
        "        exit()\n",
        "\n",
        "    display_results(papers, choice)\n",
        "\n",
        "    # --- USER DECISION ---\n",
        "    mode = input(f\"Found {len(papers)} papers. Download ALL? (y/n): \").lower()\n",
        "\n",
        "    if mode == 'y':\n",
        "        print(\"\\nğŸš€ Starting Batch Download...\")\n",
        "        for p in papers:\n",
        "            if choice == '1':\n",
        "                download_arxiv_wrapper(p)\n",
        "            else:\n",
        "                download_pubmed_wrapper(p)\n",
        "            time.sleep(DOWNLOAD_DELAY)\n",
        "    else:\n",
        "        try:\n",
        "            idx = int(input(\"Enter number to download: \")) - 1\n",
        "            if 0 <= idx < len(papers):\n",
        "                if choice == '1':\n",
        "                    download_arxiv_wrapper(papers[idx])\n",
        "                else:\n",
        "                    download_pubmed_wrapper(papers[idx])\n",
        "            else:\n",
        "                print(\"Invalid number.\")\n",
        "        except ValueError:\n",
        "            print(\"Invalid input.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8080/_page_2_Figure_0.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_page_3_Figure_0.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_page_4_Picture_0.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_page_4_Picture_1.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_page_4_Picture_2.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_page_6_Figure_0.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            },
            "http://localhost:8080/_page_6_Figure_3.jpeg": {
              "data": "",
              "headers": [
                [
                  "content-length",
                  "0"
                ]
              ],
              "ok": false,
              "status": 404,
              "status_text": ""
            }
          }
        },
        "id": "1eMWqQWCYpgI",
        "outputId": "4a12ae17-51bc-4570-bd68-b36572f588c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Previewing: LLMMAP Bimanual Robot Task Planning using Large La.md\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "# LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language\n",
              "\n",
              "**Source:** arXiv\n",
              "**Date:** 2025\n",
              "**Authors:** Kun Chu, Xufeng Zhao, Cornelius Weber, Stefan Wermter\n",
              "**Link:** http://arxiv.org/abs/2503.17309v1\n",
              "\n",
              "## Abstract\n",
              "Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, yet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zero-shot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning. However, LLMs still suffer from errors in long-horizon reasoning and from hallucinations in complex robotic tasks, lacking a guarantee of logical correctness when generating the plan. Previous works, such as LLM+P, extended LLMs with symbolic planners. However, none have been successfully applied to bimanual robots. New challenges inevitably arise in bimanual manipulation, necessitating not only effective task decomposition but also efficient task allocation. To address these challenges, this paper introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning and multi-agent planning, automating effective and efficient bimanual task planning. We conduct simulated experiments on various long-horizon manipulation tasks of differing complexity. Our method is built using GPT-4o as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-4o, V3 and also recent strong reasoning models o1 and R1. By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate the superior performance of LLM+MAP, while also providing insights into robotic reasoning. Code is available at https://github.com/Kchu/LLM-MAP.\n",
              "---\n",
              "\n",
              "# LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language\n",
              "\n",
              "Kun Chu\\*, Xufeng Zhao, Cornelius Weber, and Stefan Wermter\n",
              "\n",
              "Abstractâ€”Bimanual robotic manipulation provides significant versatility, but also presents an inherent challenge due to the complexity involved in the spatial and temporal coordination between two hands. Existing works predominantly focus on attaining human-level manipulation skills for robotic hands, vet little attention has been paid to task planning on long-horizon timescales. With their outstanding in-context learning and zeroshot generation abilities, Large Language Models (LLMs) have been applied and grounded in diverse robotic embodiments to facilitate task planning. However, LLMs still suffer from errors in long-horizon reasoning and from hallucinations in complex robotic tasks, lacking a guarantee of logical correctness when generating the plan. Previous works, such as LLM+P, extended LLMs with symbolic planners. However, none have been successfully applied to bimanual robots. New challenges inevitably arise in bimanual manipulation, necessitating not only effective task decomposition but also efficient task allocation. To address these challenges, this paper introduces LLM+MAP, a bimanual planning framework that integrates LLM reasoning and multi-agent planning, automating effective and efficient bimanual task planning. We conduct simulated experiments on various long-horizon manipulation tasks of differing complexity. Our method is built using GPT-40 as the backend, and we compare its performance against plans generated directly by LLMs, including GPT-40, V3 and also recent strong reasoning models o1 and R1. By analyzing metrics such as planning time, success rate, group debits, and planning-step reduction rate, we demonstrate superior performance of LLM+MAP, while also providing insights into robotic reasoning. Code is available at https://github.com/Kchu/LLM-MAP.\n",
              "\n",
              "#### I. Introduction\n",
              "\n",
              "Humans effortlessly perform bimanual tasks, from tying shoelaces to flipping a book's pages while holding a coffee cup, often without conscious thought. Our brain orchestrates complex spatial and temporal synchronizations, dynamically adjusting to external feedback in real time. In contrast, robots struggle with such dexterity. Even simple bimanual tasks require planning, synchronization, and complex computations. Bridging this gap is an inherent challenge in robotics [1], [2], highlighting the intricate interplay of perception, control, and adaptation.\n",
              "\n",
              "With the development of deep learning techniques, there have been some advances in dexterous bimanual manipulations, like folding shirts [3], cutting vegetables [4], scooping food [5], etc. However, most existing works concentrate on learning human-level operational skills as a whole and overlook high-level task planning. This oversight stems from the\n",
              "\n",
              "The authors are with the Knowledge Technology Group, Department of Informatics, University of Hamburg, 22527 Hamburg, Germany. E-mail: {kun.chu, xufeng.zhao, cornelius.weber, stefan.wermter}@uni-hamburg.de\n",
              "\n",
              "significant challenges of effectively allocating long-horizon tasks between dual robotic arms, such as task understanding and decomposition, subtask assignment, and proper execution sequencing.\n",
              "\n",
              "Large language models (LLMs) have shown remarkable reasoning and planning abilities in diverse fields [6] due to their training on massive textual data. Recent works have prompted LLMs in robotic task planning [7], [8], [9], [10]. LLM planning requires appropriate prompting to guide LLM to generate appropriate plans for specific embodiments and ground the plans to executable actions within the task environment. This is, however, especially challenging when dealing with complex tasks. LLMs suffer from their weakness in long-horizon reasoning and hallucinations [11], especially in spatial scenarios. Some works have explored the combination of LLMs with classical symbolic planners [12], [13] to alleviate the challenge. With LLMs serving as translators to formalize the natural language problems into some declarative language like PDDL (Planning Domain Definition Language, [14]) and ASP (Answer Set Programming, [15]), a plan is generated with logical correctness guarantee [16], [17]. Building on prior successes in integrating LLMs with symbolic planning and considering the emerging challenges in bimanual robotic scenarios, we pose the following research question: How can LLMs be integrated with multi-agent planning to achieve efficient spatial and temporal coordination in long-horizon bimanual robotic tasks?\n",
              "\n",
              "To address the question, we propose the LLM+MAP (LLM + Multi-Agent Planning with PDDL) framework, which utilizes LLMs to transform the bimanual robotic domain and problem into PDDL representation, and generates a partial-order plan through classical symbolic planners, allowing for efficient spatial and temporal coordination in bimanual manipulations. Specifically, we first define the bimanual robotic domain in PDDL, following the spatiotemporal control patterns introduced in the LABOR agent [9] for the control of NICOL, a semi-humanoid robot [18]. Based on the task description in natural language with the spatial scene information from the vision-models, the LLM is then prompted to transform task configuration into PDDL representation, compatible with the domain definition we provided. Based on the planning by symbolic solvers, a partial-order plan is generated with logical correctness and executed on the bimanual robot. We conduct extensive experiments to evaluate LLM+MAP against both large-scale coding models, GPT-40 and DeepSeek-V3 [19], and the state-\n",
              "\n",
              "<sup>\\*</sup>Corresponding author.\n",
              "\n",
              "of-the-art reasoning models, OpenAI-o1[1](#page-1-0) and DeepSeek-R1[2](#page-1-1) [20], on three task domains in the NICOL bimanual robot environment. Experimental results show that, compared to directly generating task plans with even strong reasoning models, our approach significantly outperforms in terms of reduced plan generation time, higher success rates, and more efficient task allocation.\n",
              "\n",
              "## II. RELATED WORKS\n",
              "\n",
              "In this section, we first describe task planning methods in robotics, including symbolic planners, LLM planners, and their combined applications. Then, we present a brief overview of the works in bimanual manipulation and multiagent planning.\n",
              "\n",
              "#### *A. Task Planning in Robotics*\n",
              "\n",
              "Task planning aims at generating plans, i.e., sequences of actions from a given action set, to achieve specific goals in given scenarios [21], [22].\n",
              "\n",
              "Planning with Symbolic Planners. To yield a generalpurpose automated planning system, classical planners rely on a certain type of declarative language to formalize the domains and problems. Since the introduction of STRIPS [23] in early AI research, several types of language have been proposed and widely used, including answer set programming (ASP, [15]) and planning domain definition language (PDDL, [14]). Previous works have applied symbolic planning methods in diverse tasks [22], [24], [25], [26], [27]. Task and motion planning approaches employ a high-level task planner to generate symbolic actions in discrete spaces and a low-level motion planner to generate motion trajectories in continuous space [28], [29]. Most of the planners developed in these works would generate plans with guaranteed logical correctness, yet requiring domain-specific programming languages as domain, problem, and solution representations. Inspired by these features and recent works in [12], [13], we apply PDDL to a bimanual robot scenario interacting with a human, using LLM to transform the scenario information and natural language task descriptions into PDDL representations to construct a partial-order plan using classical planners.\n",
              "\n",
              "Planning with LLMs. With the rapid developments in recent years, using natural language instructions as a prompt for LLMs to directly generate task plans has become an emergent trend in robotics. Several recent methods have leveraged LLMs as planners in diverse robotic scenarios [7], [30], [16], [9]. For instance, SayCan [7] explored utilizing LLMs to propose feasible solutions to complex tasks based on their common-sense knowledge about the world, and then ground them to specific embodiments and environments through value functions. However, LLMs suffer from their shortcomings in long-horizon reasoning and frequent hallucinations in complex tasks [31], [32], [11]. Despite some work on iterative querying LLMs by providing feedback or error messages [33], [8], [34], [9], such a strategy requires effort in careful prompt and system design to handle the feedback with LLMs.\n",
              "\n",
              "With LLMs' extraordinary in-context learning capabilities, some works investigate using LLMs to translate natural language descriptions about a task and the setting to a PDDL-readable representation, enabling classical planners to generate guaranteed solutions [12], [13], [17], [35]. Existing approaches that integrate LLMs with symbolic planning, such as ViLaIn [17], focus exclusively on scenarios requiring sequential planning, without considering the requirements of parallel execution or multi-agent planning. In contrast, our work systematically examines the distinctions between sequential and parallel planning, introducing dedicated metrics to rigorously evaluate these differences.\n",
              "\n",
              "#### *B. Bimanual Manipulation and Multi-Agent Planning*\n",
              "\n",
              "Bimanual Manipulation. With the development of deep learning techniques, some progress [2] has been made in learning dexterous manipulation skills using two grippers [4] or human-like hands [36], like scooping food [5], folding clothes [37], zipping zippers [4], etc. However, these works focus on designing learning-based systems to perform human-level operation skills, neglecting the explicit planning abilities in complex long-horizon tasks. As illustrated in these works, the high complexity associated with the variety of bimanual patterns suggests that high-level planning should be considered as well for an integrated control system design [1], [38].\n",
              "\n",
              "Bimanual and Multi-Agent Planning. Early works explored using symbolic methods for designing multi-agent systems to generate an efficient plan for complex tasks in a textual world [25], [39], [40]. Recent works have leveraged LLMs for multi-agent collaboration, where either a centralized or distributed LLM analyzes, decomposes, and assigns tasks to agent candidates in both textual [41] and robotic environments [42]. A bimanual robot can be viewed as a specialized multi-agent system, typically comprising homogeneous agents (arms) operating within a confined workspace (space around the robot body). Unlike many multi-agent systems where robots handle subtasks with varying degrees of independence, bimanual planning involves tightly coupled interactions between two arms, making it uniquely constrained in both spatial and temporal dimensions. For bimanual task planning, recent work DAG-Plan [10] employs LLMs to decompose tasks into directed acyclic graphs, assigning them to the left and right arms based on predefined rules that account for availability. However, existing approaches heavily depend on the reasoning and coding capabilities of LLMs for accurate planning, overlooking the need for computational search in an abstract space to optimize coordination. Additionally, integrating abstract representations could enhance self-verification before execution.\n",
              "\n",
              "To obtain robust and efficient bimanual plans, we format the spatial scene information for the task with linguistic task descriptions into PDDL definitions, enabling a partial-order plan to be generated for bimanual tasks, with guaranteed logical correctness and higher efficiency.\n",
              "\n",
              "<span id=\"page-1-1\"></span><span id=\"page-1-0\"></span><sup>1</sup>https://openai.com/o1/\n",
              "\n",
              "In the following, we refer to DeepSeek-R1 as R1, DeepSeek-V3 as V3, and OpenAI-o1 as o1 for brevity.\n",
              "\n",
              "![](_page_2_Figure_0.jpeg)\n",
              "\n",
              "Fig. 1: Illustrative partial-order plan for bimanual manipulation (cf. Section III), where C, A and P indicate cup, area and point respectively. Actions for the left and right hands are colored in light blue and red respectively. Two boxes shown horizontally side by side represent two actions executed in parallel.\n",
              "\n",
              "## III. METHOD\n",
              "\n",
              "<span id=\"page-2-0\"></span>Given a task description in natural language, we aim to generate a valid and efficient plan for a bimanual robot based on the initial task configurations, e.g., spatial information about the objects with respect to the robot. In this sense, this section introduces the three elements of LLM+MAP: spatial scene description, PDDL definitions with a multiagent solver for the bimanual robotic scenario, and the LLM as a PDDL writer.\n",
              "\n",
              "#### A. Spatial Scene Understanding in Bimanual Robots\n",
              "\n",
              "Based on spatio-temporal control patterns introduced in previous work [9], we define the manipulation areas for the hands based on the reachable areas:\n",
              "\n",
              "- Uncoordinated Areas: areas that only one hand can reach, whereas the other one cannot, i.e., the left area and right area respectively. In those two areas, the two hands can act independently and do not influence each other; thus, manipulations can naturally occur in parallel.\n",
              "- Coordinated Area: the area that both hands can reach, i.e., the overlap area in the middle. The two hands act and manipulate dependently in spatial and temporal relations. They can collaborate either asynchronously or synchronously â€“ an asynchronous type of control involves one hand constructing pre-conditions for the\n",
              "\n",
              "other, while a synchronous control indicates a, usually precise, mutual dependency between them.\n",
              "\n",
              "Based on this operational paradigm, for a bimanual task, we first need to figure out the areas in which the task-relevant objects and positions are located. To do this, we marked black lines on the edges of the work area to visualize and distinguish different areas. When receiving a task description, we use object-detection models like OWLv2 [43] to locate objects on the desktop according to the task-related text queries. Then, based on the object bounding box information compared with the black lines on the image, a rule-based recognizer is used to determine the area in which each object is located.\n",
              "\n",
              "#### B. Partial-order Plan Generation with LLMs and PDDL\n",
              "\n",
              "We formalize the bimanual manipulation scenario as a special case of multi-agent task planning problems. In this section, we will first informally introduce the preliminaries of PDDL, and then define the bimanual domain in PDDL. We refer the reader to other references [44], [25] for a formal treatment.\n",
              "\n",
              "#### 1) Planning Domain Definition Language\n",
              "\n",
              "Planning Domain Definition Language (PDDL, [14]) is a declarative language for standardizing the formalization of planning problems. In general, a planning problem is characterized as a tuple  $\\mathcal{T}$ :\n",
              "\n",
              "$$\\mathcal{T} = <\\mathcal{O}, \\mathcal{P}, \\mathcal{A}, \\mathcal{S}, \\mathcal{G}>,$$\n",
              "\n",
              "where  $\\mathcal{O}$  is the set of objects,  $\\mathcal{P}$  is the set of predicates,  $\\mathcal{A}$  is the set of actions,  $\\mathcal{S}$  is the initial state, and  $\\mathcal{G}$  is the goal state. A state is defined as a list of predicates applied to objects and agents, that hold. Each action is defined with parameters specifying input types and describes the pre-conditions and effects of executing such action, in terms of a series of predicates for the object inputs. The PDDL presentation of a planning problem consists of two files, 1) a *domain*, which defines objects, predicates, and actions with pre-condition and effect specifications that describe the task world, and 2) a *problem*, which includes an initial state and a desired goal state which are the sets of several specific predicates.\n",
              "\n",
              "#### 2) Multi-agent Planning\n",
              "\n",
              "In multi-agent problems, there can be a set of agents from the set of agents,  $\\mathcal{R} = \\{R_1, ..., R_N\\}$ . For an agent  $R_i \\in \\mathcal{R}$ , it can have its own set of actions  $\\mathcal{A}^i$ , predicates  $\\mathcal{P}^i$ , initi states  $\\mathcal{S}^i$ , and goal states  $\\mathcal{G}^i$ . In the cooperative multi-agent problems, the goal of each agent remains the same, i.e.,  $\\mathcal{G}^i = \\mathcal{G}$ , and only by cooperating can the goal be accomplished. In  $\\mathcal{P}^i$ , there are public predicates from  $\\mathcal{P}$ and a certain number of private predicates, i.e., environmentcommon and agent-specific properties. When executing an action inside one's own action set, one needs to consider if the value of the predicate from  $\\mathcal{P}^i$  and the environment's predicate  $\\mathcal{P}$  meet the action's pre-conditions. Besides, a cooperative action among agent i and j can only be executed if the value for several predicates from  $\\mathcal{P}^i$ ,  $\\mathcal{P}^j$  and  $\\mathcal{P}$  are hold. In this sense, the common goal can be distributed to different agents and accomplished through their cooperation\n",
              "\n",
              "![](_page_3_Figure_0.jpeg)\n",
              "\n",
              "Fig. 2: Overview of our framework. According to the spatial description of the scene, with the bimanual domain knowledge and task description, LLM+MAP generates a PDDL representation that is used for multi-agent symbolic planning. Then, a valid partial-order plan is generated and executed by the NICOL bimanual robot (see Figure [3](#page-4-0) for scenario setting).\n",
              "\n",
              "via interactions. To this end, the plan generated is a partialorder plan, which is a sequence of actions with dependencies but also flexibilities in execution orders, enabling parallel processes in multi-agent systems [40]. A dominant method for such type of problems is FMAP (Forward MultiAgent Planning, [25]), which effectively handles both cooperative and independent planning problems, outperforming existing multi-agent planning systems.\n",
              "\n",
              "Inspired by the above works, we formalize a bimanual robot to a multi-agent system, with LEFT and RIGHT as two separate agents. With private predicate *control*, each of them has their exclusive right to control the left and right hand respectively. With a cooperative goal, they will have operations in their areas, and an overlap area in between for interactions.\n",
              "\n",
              "#### *3) PDDL for Bimanual Robotic Scenario*\n",
              "\n",
              "In the bimanual robotic scenario, to specify the spatiotemporal patterns we introduced and fundamental logics about manipulations, we design a list of predicates, shown in Table [I.](#page-3-0) We define object types in O as: *hand*, *area*, *object* and *point*, and define a list of predicates for them, including their properties and relationships between them. Based on these definitions, we can define symbolic actions with parameter inputs and their pre-conditions and effects descriptions. We design atomic and fundamental skills for the homogenous robotic hand, catering for both independent (single-hand) and joint operations, shown in Table [II.](#page-3-1) For a joint operation involving both hands, the action is not defined with specific hand(s) as input parameters (since only one hand can be controlled through the agent's private predicate). Instead, the precondition requires both hands to be in an available state. Therefore, joint operations are a unique class of actions executed by both hands, despite being initiated by a single agent.\n",
              "\n",
              "#### *4) LLMs as a PDDL Writer*\n",
              "\n",
              "Under the above PPDL definition principles, we prompt the LLM to generate the initial state definition according to scene descriptions from the camera image, and the goal state definition based on the input of natural language task descrip-\n",
              "\n",
              "<span id=\"page-3-0\"></span>TABLE I: Definition of PDDL predicates for the bimanual robot scenario. By abstracting *hand*, *area*, and *object*, the predicates capture affordance properties and relationships, enabling solutions that generalize across diverse task domains.\n",
              "\n",
              "| Properties           | Relationships                  |\n",
              "|----------------------|--------------------------------|\n",
              "| control(hand)        | arm at(hand, area)             |\n",
              "| available(hand)      | arm access(hand, area)         |\n",
              "| is graspale(object)  | lifting(object, hand)          |\n",
              "| is free(object)      | object at area(object, area)   |\n",
              "| is releasable(point) | object at point(object, point) |\n",
              "| is accessible(point) | point at(point, area)          |\n",
              "\n",
              "<span id=\"page-3-1\"></span>TABLE II: Bimanual skills design. Single skills are designed independently for single hands, whereas joint skills are designed for two-hand symmetric manipulations with cooperation. Additional verification parameters are for PDDL definition to maintain appropriate pre-condition verification.\n",
              "\n",
              "| Type   | Skill(Parameters)                                 | Verif. Param.   |\n",
              "|--------|---------------------------------------------------|-----------------|\n",
              "| Single | grasp(hand, object)                               | area            |\n",
              "|        | move to(hand, object, point)                      | area            |\n",
              "|        | release(hand, object)                             | point, area     |\n",
              "|        | push(hand, object, target area)                   | source area     |\n",
              "|        | pour(hand)                                        | object1,object2 |\n",
              "|        | move above(hand, source object,<br>target object) | area            |\n",
              "|        | place(hand, source object,<br>target object)      | area            |\n",
              "| Joint  | co hold(object)                                   | -               |\n",
              "|        | co move to(point)                                 | -               |\n",
              "\n",
              "tions. Since LLMs can effectively understand and generate Python code with minimal errors, we utilize the Unified-Planning Python library [45], which streamlines problem definition and simplifies the invocation of built-in symbolic solvers, including the well-known Fast-Downward [24] and FMAP ([25]) solvers. To reduce coding errors generated by the LLM, we re-prompt it with error messages from the Python interpreter when an error occurs, enabling iterative improvements through regeneration.\n",
              "\n",
              "<span id=\"page-4-0\"></span>![](_page_4_Picture_0.jpeg)\n",
              "\n",
              "![](_page_4_Picture_1.jpeg)\n",
              "\n",
              "![](_page_4_Picture_2.jpeg)\n",
              "\n",
              "(a) ServeWater (b) ServeFruit (c) StackBlock\n",
              "\n",
              "Fig. 3: A visualization of the three task domains. In ServeWateer, the brown box is placed either in the left or right area to store the blue cup, while the cups and the human user are in random areas. In ServeFruit, the human stands exclusively in front of the overlap area to receive the bowl, while the fruits and the bowl are in random areas. In StackBlock, the blocks are distributed at random positions over the three areas, while the human user stands in front of a random area.\n",
              "\n",
              "# IV. EXPERIMENTS\n",
              "\n",
              "We conduct the following experiments to evaluate to what extent our method can achieve efficient bimanual control in robotic tasks. Specifically, we evaluate the success and efficiency of the proposed LLM+MAP in three task domains. We built LLM+MAP on GPT-4o as the backend. As a baseline, GPT-4o is prompted with bimanual domain knowledge to generate executable plans directly. The complexity of the task necessitates stronger reasoning ability for direct plan generation. Therefore, given recent advancements in reasoningcapable LLMs, we additionally conduct experiments with o1 and R1 model as two strong baselines. Besides, we include the V3 model as a counterpart to R1, as it has the same size but lacks reasoning capabilities.\n",
              "\n",
              "## *A. Environment Setup*\n",
              "\n",
              "The experiments are conducted in the CoppeliaSim[3](#page-4-1) simulator[4](#page-4-2) on the NICOL robot [18], [8], which is designed to blend social interaction with reliable object manipulation capabilities. We use one of the two cameras at its eye positions for visual perception. At the manipulation level, it is equipped with two arms, each with six degrees of freedom, and an adult-sized, five-fingered manipulator attached to it for precise manipulation of everyday objects.\n",
              "\n",
              "Three bimanual task domains are designed in a humanrobot environment: ServeWater, ServeFruit, and StackBlock. Such a setting closely resembles the real-world interaction, and also brings about the complexity and diversity of tasks through the spatial relationship between the human and the task-related objects.\n",
              "\n",
              "In the ServeWater task domain, there are two cups on the work table, an empty yellow cup and a water-filled blue cup, and the task requires the robot to serve the water in the yellow cup to the human, while putting the blue cup to a specified store point on the brown box, which is randomly located in the left or right area. In the ServeFruit task domain, there is a banana, an apple, and a big red bowl, and the task requires serving the fruits with a bowl to the human. It should be noted that the bowl is not graspable with one single hand, and the symmetric manipulations for two hands can only be done in the overlap area. In this sense, the bowl should be pushed to the overlap area so that it can be held by both hands. In the StackBlock task domain, there are several blocks with different colors randomly located on the table, and the task goal is to stack one or two piles with specific blocks in a certain order. We design the task's configurations in two aspects: the total number of blocks to be stacked, and the number of cubes in each pile. Specifically, the task requires the robot to stack the selected four or five blocks into a specific pile in terms of [(2, 2),(3, 1),(4, 0)] or [(2, 3),(4, 1),(5, 0)], respectively. An example of a task goal with (4, 1) in natural language is, the task is to stack the selected five blocks in two piles on the tray right in front of the human, where one pile is yellow over red over purple over black and one pile is green.\n",
              "\n",
              "## <span id=\"page-4-4\"></span>*B. Metrics*\n",
              "\n",
              "To examine the efficacy and efficiency of generated plans, we compare our proposed methods with baselines along the following three metrics:\n",
              "\n",
              "- Planning Time (PT). For LLM-direct task planning, the time cost depends solely on the inference time of certain LLMs, while the planning time of our method is composed of LLM inference time and symbolic planning time.\n",
              "- Success Rate (SR), which reveals the overall ability for task completion.\n",
              "- Group Debits (GD), a metric to compare the planning efficiency of models.[5](#page-4-3) Specifically, we set the debits of the \"champion\" model (the one that uses the fewest\n",
              "\n",
              "<span id=\"page-4-3\"></span><sup>5</sup>Since the minimum number of task completion steps varies across runs, due of the randomness in initialization, a comparative metric avoids the need to know the number of steps of the optimal plan.\n",
              "\n",
              "<span id=\"page-4-2\"></span><span id=\"page-4-1\"></span><sup>3</sup>https://www.coppeliarobotics.com/\n",
              "\n",
              "<sup>4</sup> In the simulation, we do not incorporate physical randomness (e.g., certain probability of failure) to mimic real-world execution, as our primary focus is on the planning domain. Introducing physical randomness would add additional uncertainty, which could complicate the analysis.\n",
              "\n",
              "planning steps) to 0. Other models are assigned debits according to the number of planning steps exceeding those of the champion (higher debits mean worse performance).\n",
              "\n",
              "#### *C. Results*\n",
              "\n",
              "Planning Time. As is shown in Table [III,](#page-5-0) we have several immediate observations: (1) the PT of GPT-4o, V3 direct and LLM+MAP are dramatically shorter than the other two advanced reasoning models, o1 and R1. (2) The LLM inference time increases across all models as task complexity grows. (3) In analyzing the PT distribution for LLM+MAP, the LLM inference time remains relatively stable, while the symbolic planning time for simpler tasks is sufficiently small (smaller than the LLM inference time), but increases considerably for more complex tasks with a larger search space. In practice,\n",
              "\n",
              "<span id=\"page-5-0\"></span>TABLE III: Task Planning Time (s) for different models. For LLM+MAP, we also provide the time spent on each module, including LLM inference time for code generation and PDDL planning time.\n",
              "\n",
              "| Model         | ServeWater    | ServeFruit    | StackBlock-4  | StackBlock-5  |\n",
              "|---------------|---------------|---------------|---------------|---------------|\n",
              "| gpt-4o direct | 5.33 Â±6.70    | 4.17 Â±1.35    | 7.82 Â±2.46    | 11.17 Â±20.65  |\n",
              "| V3 direct     | 10.80 Â±1.90   | 10.41 Â±2.03   | 14.00 Â±2.41   | 17.20 Â±2.78   |\n",
              "| R1 direct     | 122.42 Â±40.16 | 144.42 Â±70.94 | 220.85 Â±72.68 | 168.28 Â±38.59 |\n",
              "| o1 direct     | 104.29 Â±71.69 | 77.68 Â±31.62  | 96.85 Â±51.00  | 66.38 Â±17.96  |\n",
              "| LLM+MAP       | 11.34 Â±6.73   | 7.75 Â±0.91    | 34.97 Â± 21.53 | 62.75 Â±26.05  |\n",
              "| âˆˆ LLM         | 9.55 Â±6.70    | 6.00 Â±0.91    | 13.08 Â±4.19   | 18.23 Â±8.11   |\n",
              "| âˆˆ MAP         | 1.80 Â±0.49    | 1.74 Â±0.25    | 21.89 Â±20.97  | 44.52 Â±23.40  |\n",
              "\n",
              "multi-agent planning can be time-consuming with the FMAP solver, so we set a *timeout* to avoid excessively long computation times. As an alternative, we convert it to a singlerobot task, allowing for a feasible solution with the BFWS solver [46]. According to the bimanual characteristics, the generated plan is then post-processed as a partial-order plan using automated graph tools.\n",
              "\n",
              "Success Rate. From the experimental results presented in Table [IV,](#page-5-1) we have the following findings: (1) It is clear that our proposed method achieves the highest performance out of all tasks, thanks to the integration of LLM coding and multi-agent planning. This result is impressive because the original performance of GPT-4o, as a base LLM with specifically tuning in the pursuit of strong reasoning ability, performs poorly while our method â€“ built on top of this base model â€“ outperforms. GPT-4o's strength in in-context understanding minimizes errors during the generation of PDDL definitions. In the StackBlock domain, such errors primarily result from GPT-4o defining the task goal with an incorrect block order, which consequently leads to execution failures in the actual task configuration. (2) Long-horizon robotic tasks require strong reasoning ability for correct task completions, specifically tuned reasoning LLMs, o1 and R1, trade inference time compute (indicating both higher cost in both time and expense) with a better reasoning competence, resulting in higher SR in our experiments. (3) Comparing the results of V3 and R1, it is enlightening that, *despite having the same parameter scale*, R1 dramatically outperforms V3. This suggests that strong reasoning ability is crucial for solving long-horizon tasks, particularly when the base language model is not integrated with a planning mechanism like LLM+MAP.\n",
              "\n",
              "<span id=\"page-5-1\"></span>TABLE IV: Success rate (%) of plan execution across the task domains.\n",
              "\n",
              "| Model         | ServeWater | ServeFruit | StackBlock-4 | StackBlock-5 |\n",
              "|---------------|------------|------------|--------------|--------------|\n",
              "| gpt-4o direct | 2          | 13         | 2            | 0            |\n",
              "| V3 direct     | 2          | 6          | 6            | 1            |\n",
              "| R1 direct     | 67         | 63         | 94           | 77           |\n",
              "| o1 direct     | 84         | 82         | 95           | 88           |\n",
              "| LLM+MAP       | 100        | 100        | 96           | 97           |\n",
              "\n",
              "Group Debits. As is discussed in Subsection [IV-B,](#page-4-4) GD is a metric to compare the planning efficiency of models. From Figure [4,](#page-6-0) we find that (1) LLM+MAP almost dominates the competitions, especially in easier tasks, with mass mainly distributed around 0 debits (relative optimal, i.e. winner with minimal planning steps among competitors). (2) With the growth of task complexity, the GD of reasoning models is comparable to the multi-agent planning results, indicating that stronger reasoning ability helps a comprehensive understanding of the temporal and spatial resilience of dual hands. However, for easier tasks, plans generated by those strong reasoning models are far from being efficient, we hypothesize that this non-efficiency may stem from the overthink [47] of current reasoning models, especially of the R1 model, which takes excessive reasoning time (cf. Table [III\\)](#page-5-0) and outputs wordy content.\n",
              "\n",
              "Discussion. The combination of the above metrics provides additional insights. Although the SR of the R1 model is lower than that of the o1 model, its GD scores higher, indicating greater efficiency in terms of bimanual coordination and suggesting a superior temporal understanding. In contrast, the o1 model appears to have a stronger spatial understanding, which is crucial for successful task completion. The decoupling of temporal and spatial dimensions, along with the corresponding investigation, is left as a direction for future work.\n",
              "\n",
              "# *D. Ablation Study*\n",
              "\n",
              "To investigate the efficacy of multi-agent PDDL planning in comparison to traditional planning, which treats configurations of both hands as a unified collection of presets of a single agent, we conduct an ablation study by removing MAP component and only use BFWS [46] as its solver to generate sequential plan, resulting in an adapted[6](#page-5-2) implementation of LLM+P [12]. We compute the Planning Step Reduction Rate (PSRR) as\n",
              "\n",
              "$$\\mathrm{PSRR} = \\frac{N_{\\mathrm{+P}} - N_{\\mathrm{+MAP}}}{N_{\\mathrm{+P}}} \\times 100\\%, \\label{eq:psrr}$$\n",
              "\n",
              "<span id=\"page-5-2\"></span><sup>6</sup>Note that the method introduced in LLM+P never considers dual robotic arm or other forms of multi-agent systems, we thus adapt it onto bimanual setting with modification ranging from skill design to domain definition. Implementing details can be found in the code page.\n",
              "\n",
              "<span id=\"page-6-0\"></span>![](_page_6_Figure_0.jpeg)\n",
              "\n",
              "Fig. 4: The *Group Debits* statistics among successful tasks in three domains, the smaller the better.\n",
              "\n",
              "where N+P and N+MAP are the number of planning steps for LLM+P and LLM+MAP respectively. As shown in Figure [5](#page-6-1) (across successful tasks in 100 runs), compared to the sequential plans generated by LLM+P, our method facilitates more parallel task allocation for both hands, resulting in a significant improvement in overall efficiency.\n",
              "\n",
              "<span id=\"page-6-1\"></span>![](_page_6_Figure_3.jpeg)\n",
              "\n",
              "Fig. 5: Planning Step Reduction Rate (%) of LLM+MAP over LLM+P, showcasing the improved efficiency of Multiagent planning.\n",
              "\n",
              "# V. CONCLUSION\n",
              "\n",
              "We propose LLM+MAP, which formulates the bimanual robot task planning problem as a special form of multiagent planning, leveraging the coding and reasoning ability of LLMs and formalized language representations for efficient multi-agent planning. Specifically, vision models transform spatial information and task descriptions into PDDL representations in bimanual robot scenarios, achieving efficient spatial and temporal coordination with guaranteed correctness and optimality by symbolic planners. Extensive experiments on three task domains showcase that our framework outperforms planning results with GPT-4o, V3 and even strong reasoning models o1 and R1, in terms of higher success rate and efficiency with less plan generation time.\n",
              "\n",
              "Limitations and Future Work. While our main focus in this paper is not on the acquisition and design of bimanual skills, the execution of the underlying task assumes that existing motion primitives serve the purpose well, raising a certain gap when transferring to the real world. Integrating with learning-based bimanual robotic skills will be our main focus in the future. We have used the LLM+MAP framework for merely two agents, i.e., the robot's hands, while the feasibility and effectivity of extending our framework to control a larger number of agents are still to be investigated. Hierarchical planning with LLM bootstrapping can be a viable direction to explore for task planning in large-scale multi-robot tasks. Additionally, considering the dynamic nature of the world, future work includes incorporating action durations or costs for more fine-grained bimanual planning, enabling adaptive re-planning in response to action failures or unexpected environmental changes, and improving the verification of action preconditions and effects, etc.\n",
              "\n",
              "### ACKNOWLEDGMENT\n",
              "\n",
              "The authors gratefully acknowledge support from the Horizon Europe project TERAIS and the MSCA Doctoral Network TRAIL. Additionally, they express their gratitude to OpenAI's Researcher Access Program for generously providing API tokens.\n",
              "\n",
              "# REFERENCES\n",
              "\n",
              "- [1] F. Krebs and T. Asfour, \"A bimanual manipulation taxonomy,\" *IEEE Robotics and Automation Letters*, vol. 7, no. 4, pp. 11 031â€“11 038, 2022.\n",
              "- [2] M. Drolet, S. Stepputtis, S. Kailas, A. Jain, J. Peters, S. Schaal, and H. B. Amor, \"A comparison of imitation learning algorithms for bimanual manipulation,\" *IEEE Robotics and Automation Letters*, vol. 9, no. 10, pp. 8579â€“8586, 2024.\n",
              "- [3] K. Shaw, Y. Li, J. Yang, M. K. Srirama, R. Liu, H. Xiong, R. Mendonca, and D. Pathak, \"Bimanual dexterity for complex tasks,\" in *The 8th Annual Conference on Robot Learning (CoRL)*, 2024.\n",
              "- [4] J. Grannen, Y. Wu, B. Vu, and D. Sadigh, \"Stabilize to act: Learning to coordinate for bimanual manipulation,\" in *The 7th Conference on Robot Learning (CoRL)*. PMLR, 2023, pp. 563â€“576.\n",
              "- [5] J. Grannen, Y. Wu, S. Belkhale, and D. Sadigh, \"Learning bimanual scooping policies for food acquisition,\" in *Proceedings of The 6th Conference on Robot Learning*, ser. Proceedings of Machine Learning Research, K. Liu, D. Kulic, and J. Ichnowski, Eds., vol. 205. PMLR, 14â€“18 Dec 2023, pp. 1510â€“1519. [Online]. Available: <https://proceedings.mlr.press/v205/grannen23a.html>\n",
              "- [6] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, *et al.*, \"A survey of large language models,\" *arXiv preprint arXiv:2303.18223*, vol. 1, no. 2, 2023.\n",
              "- [7] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian, *et al.*, \"Do as I can, not as I say: Grounding language in robotic affordances,\" in *Proceedings of The 6th Conference on Robot Learning (CoRL)*. PMLR, 2023, pp. 287â€“318.\n",
              "- [8] X. Zhao, M. Li, C. Weber, M. B. Hafez, and S. Wermter, \"Chat with the environment: Interactive multimodal perception using large language models,\" in *2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2023, pp. 3590â€“3596.\n",
              "- [9] K. Chu, X. Zhao, C. Weber, M. Li, W. Lu, and S. Wermter, \"Large language models for orchestrating bimanual robots,\" in *2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)*, 2024, pp. 328â€“334.\n",
              "\n",
              "- [10] Z. Gao, Y. Mu, J. Qu, M. Hu, L. Guo, P. Luo, and Y. Lu, \"DAGplan: Generating directed acyclic dependency graphs for dual-arm cooperative planning,\" *arXiv preprint arXiv:2406.09953*, 2024.\n",
              "- [11] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, *et al.*, \"A survey on large language model based autonomous agents,\" *Frontiers of Computer Science*, vol. 18, no. 6, p. 186345, 2024.\n",
              "- [12] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, \"LLM+P Empowering large language models with optimal planning proficiency,\" *arXiv preprint arXiv:2304.11477*, 2023.\n",
              "- [13] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, \"Translating natural language to planning goals with large-language models,\" *arXiv preprint arXiv:2302.05128*, 2023.\n",
              "- [14] C. Aeronautiques, A. Howe, C. Knoblock, I. D. McDermott, A. Ram, M. Veloso, D. Weld, D. W. Sri, A. Barrett, D. Christianson, *et al.*, \"PDDLâ€” the planning domain definition language,\" *Technical Report, Tech. Rep.*, 1998.\n",
              "- [15] V. Lifschitz, \"Answer set programming and plan generation,\" *Artificial Intelligence*, vol. 138, no. 1-2, pp. 39â€“54, 2002.\n",
              "- [16] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, \"Task and motion planning with large language models for object rearrangement,\" in *2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. IEEE, 2023, pp. 2086â€“2092.\n",
              "- [17] K. Shirai, C. C. Beltran-Hernandez, M. Hamaya, A. Hashimoto, S. Tanaka, K. Kawaharazuka, K. Tanaka, Y. Ushiku, and S. Mori, \"Vision-language interpreter for robot task planning,\" in *2024 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE, 2024, pp. 2051â€“2058.\n",
              "- [18] M. Kerzel, P. Allgeuer, E. Strahl, N. Frick, J.-G. Habekost, M. Eppe, and S. Wermter, \"NICOL: A neuro-inspired collaborative semihumanoid robot that bridges social interaction and reliable manipulation,\" *IEEE Access*, vol. 11, pp. 123 531â€“123 542, 2023.\n",
              "- [19] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, *et al.*, \"Deepseek-v3 technical report,\" *arXiv preprint arXiv:2412.19437*, 2024.\n",
              "- [20] DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, *et al.*, \"DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning,\" 2025. [Online]. Available: <https://arxiv.org/abs/2501.12948>\n",
              "- [21] A. E. Gerevini, \"An introduction to the planning domain definition language (PDDL): Book review,\" *Artificial Intelligence*, vol. 280, p. 103221, 2020.\n",
              "- [22] Y.-q. Jiang, S.-q. Zhang, P. Khandelwal, and P. Stone, \"Task planning in robotics: an empirical comparison of PDDL-and ASP-based systems,\" *Frontiers of Information Technology & Electronic Engineering*, vol. 20, pp. 363â€“373, 2019.\n",
              "- [23] R. E. Fikes and N. J. Nilsson, \"STRIPS: A new approach to the application of theorem proving to problem solving,\" *Artificial Intelligence*, vol. 2, no. 3-4, pp. 189â€“208, 1971.\n",
              "- [24] M. Helmert, \"The fast downward planning system,\" *Journal of Artificial Intelligence Research*, vol. 26, pp. 191â€“246, 2006.\n",
              "- [25] A. Torreno, E. Onaindia, and O. Sapena, \"FMAP: Distributed cooperative multi-agent planning,\" *Applied Intelligence*, vol. 41, pp. 606â€“626, 2014.\n",
              "- [26] Y. Ding, X. Zhang, X. Zhan, and S. Zhang, \"Task-motion planning for safe and efficient urban driving,\" in *2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, 2020, pp. 2119â€“ 2125.\n",
              "- [27] Y. Jiang, H. Yedidsion, S. Zhang, G. Sharon, and P. Stone, \"Multi-robot planning with conflicts and synergies,\" *Autonomous Robots*, vol. 43, no. 8, pp. 2011â€“2032, 2019.\n",
              "- [28] L. P. Kaelbling and T. Lozano-Perez, \"Integrated task and motion Â´ planning in belief space,\" *The International Journal of Robotics Research*, vol. 32, no. 9-10, pp. 1194â€“1227, 2013.\n",
              "- [29] Z. Jiao, Z. Zhang, W. Wang, D. Han, S.-C. Zhu, Y. Zhu, and H. Liu, \"Efficient task planning for mobile manipulation: a virtual kinematic chain perspective,\" in *2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. IEEE, 2021, pp. 8288â€“8294.\n",
              "- [30] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, \"Language models as zero-shot planners: Extracting actionable knowledge for embodied agents,\" in *International Conference on Machine Learning (ICML)*. PMLR, 2022, pp. 9118â€“9147.\n",
              "\n",
              "- [31] X. Zhao, M. Li, W. Lu, C. Weber, J. H. Lee, K. Chu, and S. Wermter, \"Enhancing zero-shot chain-of-thought reasoning in large language models through logic,\" in *Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)*. ELRA and ICCL, May 2024, pp. 6144â€“6166.\n",
              "- [32] K. Stechly, K. Valmeekam, and S. Kambhampati, \"On the selfverification limitations of large language models on reasoning and planning tasks,\" *arXiv preprint arXiv:2402.08115*, 2024.\n",
              "- [33] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma, Y. Liang, and T. CraftJarvis, \"Describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents,\" in *Proceedings of the 37th International Conference on Neural Information Processing Systems (NeurIPS)*, ser. NIPS '23. Red Hook, NY, USA: Curran Associates Inc., 2023.\n",
              "- [34] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf, \"SayPlan: Grounding large language models using 3D scene graphs for scalable robot task planning,\" in *7th Annual Conference on Robot Learning (CoRL)*, 2024.\n",
              "- [35] Y. Chen, J. Arkin, C. Dawson, Y. Zhang, N. Roy, and C. Fan, \"AutoTAMP: Autoregressive task and motion planning with LLMs as translators and checkers,\" in *2024 IEEE International Conference on Robotics and Automation (ICRA)*. IEEE, 2024, pp. 6695â€“6702.\n",
              "- [36] Y. Chen, T. Wu, S. Wang, X. Feng, J. Jiang, Z. Lu, S. McAleer, H. Dong, S.-C. Zhu, and Y. Yang, \"Towards human-level bimanual dexterous manipulation with reinforcement learning,\" *Advances in Neural Information Processing Systems (NeurIPS)*, vol. 35, pp. 5150â€“ 5163, 2022.\n",
              "- [37] Y. Avigal, L. Berscheid, T. Asfour, T. Kroger, and K. Goldberg, Â¨ \"Speedfolding: Learning efficient bimanual folding of garments,\" in *2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*. IEEE, 2022, pp. 1â€“8.\n",
              "- [38] C. Smith, Y. Karayiannidis, L. Nalpantidis, X. Gratal, P. Qi, D. V. Dimarogonas, and D. Kragic, \"Dual arm manipulationâ€”a survey,\" *Robotics and Autonomous systems*, vol. 60, no. 10, pp. 1340â€“1353, 2012.\n",
              "- [39] J. Cox and E. Durfee, \"Efficient and distributable methods for solving the multiagent plan coordination problem,\" *Multiagent and Grid Systems*, vol. 5, no. 4, pp. 373â€“408, 2009.\n",
              "- [40] M. Ghallab, D. Nau, and P. Traverso, *Automated Planning: theory and practice*. Elsevier, 2004.\n",
              "- [41] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, L. Xiao, C. Wu, and J. Schmidhuber, \"MetaGPT: Meta programming for a multi-agent collaborative framework,\" in *The Twelfth International Conference on Learning Representations (ICLR)*, 2024.\n",
              "- [42] S. S. Kannan, V. L. N. Venkatesh, and B.-C. Min, \"SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models,\" in *2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, Oct. 2024, pp. 12 140â€“12 147.\n",
              "- [43] M. Minderer, A. Gritsenko, and N. Houlsby, \"Scaling open-vocabulary object detection,\" *Advances in Neural Information Processing Systems (NeurIPS)*, vol. 36, pp. 72 983â€“73 007, 2023.\n",
              "- [44] D. L. Kovacs *et al.*, \"A multi-agent extension of PDDL3.1,\" in *ICAPS 2012 Proceedings of the 3rd Workshop on the International Planning Competition (WS-IPC 2012)*, 2012, pp. 19â€“37.\n",
              "- [45] A. Micheli, A. Bit-Monnot, G. Roger, E. Scala, A. Valentini, Â¨ L. Framba, A. Rovetta, A. Trapasso, L. Bonassi, A. E. Gerevini, L. Iocchi, F. Ingrand, U. Kockemann, F. Patrizi, A. Saetti, I. Serina, Â¨ and S. Stock, \"Unified planning: Modeling, manipulating and solving AI planning problems in python,\" *SoftwareX*, vol. 29, p. 102012, 2025. [Online]. Available: [https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S2352711024003820) [S2352711024003820](https://www.sciencedirect.com/science/article/pii/S2352711024003820)\n",
              "- [46] G. Frances, H. Geffner, N. Lipovetzky, and M. Ramirez, \"Best-first Â´ width search in the IPC 2018: Complete, simulated, and polynomial variants,\" *IPC-9 Planner Abstracts*, pp. 23â€“27, 2018.\n",
              "- [47] X. Chen, J. Xu, T. Liang, Z. He, J. Pang, D. Yu, L. Song, Q. Liu, M. Zhou, Z. Zhang, *et al.*, \"Do NOT think that much for 2+ 3=? On the overthinking of o1-like LLMs,\" *arXiv preprint arXiv:2412.21187*, 2024."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def preview_first_markdown():\n",
        "    folder = \"papers_markdown\"\n",
        "\n",
        "    # 1. Check if folder exists\n",
        "    if not os.path.exists(folder):\n",
        "        print(f\"âŒ Folder '{folder}' not found. Run the fetcher first!\")\n",
        "        return\n",
        "\n",
        "    # 2. Find .md files\n",
        "    files = [f for f in os.listdir(folder) if f.endswith('.md')]\n",
        "\n",
        "    if not files:\n",
        "        print(f\"âš ï¸ No markdown files found in '{folder}'.\")\n",
        "        return\n",
        "\n",
        "    # 3. Read and Display the first one\n",
        "    first_file = files[0]\n",
        "    path = os.path.join(folder, first_file)\n",
        "\n",
        "    print(f\"ğŸ“„ Previewing: {first_file}\\n\" + \"=\"*50)\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Render Markdown in Jupyter/Colab\n",
        "    display(Markdown(content))\n",
        "\n",
        "# Run the preview\n",
        "preview_first_markdown()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "L_52LV-QaPJY",
        "outputId": "64e4ab4d-fb08-47f7-a9e2-9f9385579628"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'llama_index'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1372305228.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQueryEngineTool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToolMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_engine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouterQueryEngine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMSingleSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 1. Define Tool A: The \"Specific Detail\" Finder (Standard RAG)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llama_index'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "# 1. Define Tool A: The \"Specific Detail\" Finder (Standard RAG)\n",
        "vector_tool = QueryEngineTool(\n",
        "    query_engine=vector_query_engine, # Your existing RAG engine\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"specific_finder\",\n",
        "        description=\"Useful for finding specific facts, numbers, or details in the papers.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 2. Define Tool B: The \"Big Picture\" Summarizer\n",
        "summary_tool = QueryEngineTool(\n",
        "    query_engine=summary_query_engine, # Your summary engine\n",
        "    metadata=ToolMetadata(\n",
        "        name=\"general_summarizer\",\n",
        "        description=\"Useful for high-level summaries or understanding what the paper is about.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "# 3. Create the Router (The Brain)\n",
        "# It uses an LLM to read the description and pick the right tool dynamically.\n",
        "router = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        vector_tool,\n",
        "        summary_tool,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Use it\n",
        "response = router.query(\"What are the specific limitations mentioned in the lung cancer paper?\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "article",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
