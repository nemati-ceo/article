{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe036ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: paperflow 0.1.7\n",
      "Uninstalling paperflow-0.1.7:\n",
      "  Successfully uninstalled paperflow-0.1.7\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall paperflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f9980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting paperflow\n",
      "  Downloading paperflow-0.1.8-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (2.12.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (0.28.1)\n",
      "Requirement already satisfied: arxiv>=2.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (2.3.1)\n",
      "Requirement already satisfied: biopython>=1.80 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (1.86)\n",
      "Requirement already satisfied: requests>=2.28.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (2.32.5)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (0.9.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from arxiv>=2.0.0->paperflow) (6.0.12)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from feedparser~=6.0.10->arxiv>=2.0.0->paperflow) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (2025.11.12)\n",
      "Requirement already satisfied: numpy in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from biopython>=1.80->paperflow) (2.3.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpx>=0.25.0->paperflow) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpx>=0.25.0->paperflow) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->paperflow) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic>=2.0->paperflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic>=2.0->paperflow) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.0->paperflow) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic>=2.0->paperflow) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anyio->httpx>=0.25.0->paperflow) (1.3.1)\n",
      "Downloading paperflow-0.1.8-py3-none-any.whl (30 kB)\n",
      "Installing collected packages: paperflow\n",
      "Successfully installed paperflow-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install paperflow[extraction-all,rag,providers] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "051ad207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
      "â”‚                    paperflow                                  â”‚\n",
      "â”‚            Academic paper ingestion                           â”‚\n",
      "â”‚                                                               â”‚\n",
      "â”‚  search â†’ download â†’ extract Markditdown                      â”‚\n",
      "â”‚                                                               â”‚\n",
      "â”‚                       version: 0.1.8                        â”‚\n",
      "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.1.8'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'd:/NAI_Project/paperflow/src')\n",
    "\n",
    "# Remove from cache to force reload\n",
    "modules_to_remove = [k for k in sys.modules.keys() if k.startswith('paperflow')]\n",
    "for mod in modules_to_remove:\n",
    "    del sys.modules[mod]\n",
    "\n",
    "import paperflow\n",
    "\n",
    "# Force reload pipeline\n",
    "import importlib\n",
    "importlib.reload(paperflow.pipeline)\n",
    "\n",
    "paperflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3d663",
   "metadata": {},
   "source": [
    "# PaperFlow: Complete Academic Paper Processing Demo\n",
    "\n",
    "PaperFlow is a unified pipeline for academic paper ingestion, extraction, and RAG (Retrieval-Augmented Generation). It allows you to:\n",
    "\n",
    "- **Search** across multiple academic sources (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- **Download** PDFs automatically\n",
    "- **Extract** text and structure from PDFs\n",
    "- **Chunk** content for RAG applications\n",
    "- **Embed** and store in vector databases\n",
    "- **Query** papers using natural language\n",
    "\n",
    "## Features\n",
    "\n",
    "- ğŸ” Multi-source search (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- ğŸ“¥ Automatic PDF downloading\n",
    "- ğŸ“„ Advanced PDF text extraction (Marker AI, Docling, MarkItDown)\n",
    "- âœ‚ï¸ Intelligent text chunking\n",
    "- ğŸ§  Vector embeddings for RAG\n",
    "- ğŸ’¾ ChromaDB integration\n",
    "- ğŸ”— LangChain compatibility\n",
    "- ğŸ“Š Tabular result display\n",
    "- ğŸ’» Command-line interface\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the latest version of PaperFlow with all optional dependencies.\n",
    "```python\n",
    "pipeline = PaperPipeline(gpu=USE_GPU, pdf_dir=PDF_DIR)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3cb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Loading Marker AI models...\n",
      "âœ… Marker AI loaded\n",
      "Searching for papers on transformers...\n",
      "Found 3 papers in 373ms\n",
      "Sources: ['openalex']\n",
      "\n",
      "+-----+---------------------------------------+-------------------------------+--------+----------+-----------------+\n",
      "|   # | Title                                 | Authors                       |   Year | Source   | Link/ID         |\n",
      "+=====+=======================================+===============================+========+==========+=================+\n",
      "|   1 | CrossViT: Cross-Attention Multi-Scale | Chun-Fu Richard Chen, Quanfu  |   2021 | openalex | 10.1109/iccv489 |\n",
      "|     | Vision Transformer for...             | Fan, Rameswar Panda           |        |          | 22.2021.00041   |\n",
      "+-----+---------------------------------------+-------------------------------+--------+----------+-----------------+\n",
      "|   2 | BiFormer: Vision Transformer with Bi- | Lei Zhu, Xinjiang Wang,       |   2023 | openalex | 10.1109/cvpr527 |\n",
      "|     | Level Routing Attention               | Zhanghan Ke et al.            |        |          | 29.2023.00995   |\n",
      "+-----+---------------------------------------+-------------------------------+--------+----------+-----------------+\n",
      "|   3 | Vision Transformer with Deformable    | Zhuofan Xia, Xuran Pan, Shiji |   2022 | openalex | 10.1109/cvpr526 |\n",
      "|     | Attention                             | Song et al.                   |        |          | 88.2022.00475   |\n",
      "+-----+---------------------------------------+-------------------------------+--------+----------+-----------------+\n",
      "\n",
      "Downloading all papers...\n",
      "Downloading paper 1: CrossViT: Cross-Attention Multi-Scale Vision Trans...\n",
      "  - PDF saved: None\n",
      "\n",
      "Downloading paper 2: BiFormer: Vision Transformer with Bi-Level Routing...\n",
      "  - PDF saved: None\n",
      "\n",
      "Downloading paper 3: Vision Transformer with Deformable Attention...\n",
      "  - PDF saved: None\n",
      "\n",
      "Extracting all papers...\n",
      "Extracting paper 1: CrossViT: Cross-Attention Multi-Scale Vision Trans...\n",
      "  - Sections: 0, Chunks: 1\n",
      "\n",
      "Extracting paper 2: BiFormer: Vision Transformer with Bi-Level Routing...\n",
      "  - Sections: 0, Chunks: 1\n",
      "\n",
      "Extracting paper 3: Vision Transformer with Deformable Attention...\n",
      "  - Sections: 0, Chunks: 1\n",
      "\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have CUDA GPU and want faster extraction\n",
    "PDF_DIR = './test_pdfs'\n",
    "\n",
    "# Choose PDF extraction backend:\n",
    "# - \"auto\": Try marker â†’ docling â†’ markitdown (recommended)\n",
    "# - \"marker\": High quality, best for academic papers, GPU support\n",
    "# - \"docling\": Good table/figure extraction, IBM, GPU support  \n",
    "# - \"markitdown\": Lightweight, fast, CPU only, Microsoft\n",
    "EXTRACTION_BACKEND = \"auto\"\n",
    "\n",
    "print(f\"GPU acceleration: {'Enabled' if USE_GPU else 'Disabled'}\")\n",
    "print(f\"PDF directory: {PDF_DIR}\")\n",
    "print(f\"Extraction backend: {EXTRACTION_BACKEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0ffd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Create pipeline with selected backend\n",
    "pipeline = PaperPipeline(\n",
    "    gpu=USE_GPU, \n",
    "    pdf_dir=PDF_DIR,\n",
    "    extraction_backend=EXTRACTION_BACKEND\n",
    ")\n",
    "\n",
    "print(f\"âœ… Pipeline created with {pipeline._extractor.active_backend} backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aee8d7",
   "metadata": {},
   "source": [
    "## 1. arXiv Provider Demo\n",
    "\n",
    "Search for computer science papers on arXiv, the preprint server for physics, mathematics, computer science, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ce1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search arXiv\n",
    "print('ğŸ” Searching arXiv for \"transformer attention\" papers...')\n",
    "results = pipeline.search('transformer attention mechanism', sources=['arxiv'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {results.total_found} papers in {results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045e7d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results\n",
    "if results.papers:\n",
    "    print(\"\\nğŸ“‹ arXiv Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âŒ No papers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a402a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results in JSON format\n",
    "if results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ arXiv Search Results in JSON:\")\n",
    "    print(json.dumps(results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3ee86",
   "metadata": {},
   "source": [
    "## 2. PubMed Provider Demo\n",
    "\n",
    "Search for biomedical research papers in PubMed, the premier database for biomedical literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search PubMed\n",
    "print('ğŸ” Searching PubMed for \"CRISPR gene editing\" papers...')\n",
    "pubmed_results = pipeline.search('CRISPR gene editing therapy', sources=['pubmed'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {pubmed_results.total_found} papers in {pubmed_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd0802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results\n",
    "if pubmed_results.papers:\n",
    "    print(\"\\nğŸ“‹ PubMed Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(pubmed_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results in JSON format\n",
    "if pubmed_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ PubMed Search Results in JSON:\")\n",
    "    print(json.dumps(pubmed_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a239d56",
   "metadata": {},
   "source": [
    "## 3. Semantic Scholar Provider Demo\n",
    "\n",
    "Search using Semantic Scholar's AI-powered academic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819d9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Semantic Scholar\n",
    "print('ğŸ” Searching Semantic Scholar for \"large language models\"...')\n",
    "sem_results = pipeline.search('large language models GPT', sources=['semantic_scholar'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {sem_results.total_found} papers in {sem_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87cd046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results\n",
    "if sem_results.papers:\n",
    "    print(\"\\nğŸ“‹ Semantic Scholar Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(sem_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results in JSON format\n",
    "if sem_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ Semantic Scholar Search Results in JSON:\")\n",
    "    print(json.dumps(sem_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e78dcf",
   "metadata": {},
   "source": [
    "## 4. OpenAlex Provider Demo\n",
    "\n",
    "Search the OpenAlex catalog, which covers millions of scholarly works from all disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afbd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search OpenAlex\n",
    "print('ğŸ” Searching OpenAlex for \"climate change adaptation\"...')\n",
    "openalex_results = pipeline.search('climate change adaptation strategies', sources=['openalex'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {openalex_results.total_found} papers in {openalex_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37493b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results\n",
    "if openalex_results.papers:\n",
    "    print(\"\\nğŸ“‹ OpenAlex Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(openalex_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results in JSON format\n",
    "if openalex_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ OpenAlex Search Results in JSON:\")\n",
    "    print(json.dumps(openalex_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3efa84",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline Demo\n",
    "\n",
    "Demonstrate the full pipeline: search â†’ download â†’ extract â†’ chunk â†’ embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline demonstration\n",
    "print(\"ğŸš€ Running complete pipeline...\")\n",
    "\n",
    "# 1. Search\n",
    "print(\"1. Searching for papers...\")\n",
    "search_results = pipeline.search(\"neural networks\", sources=[\"arxiv\"], max_results=1)\n",
    "\n",
    "if search_results.papers:\n",
    "    paper_dict = search_results.papers[0]\n",
    "    \n",
    "    # 2. Download\n",
    "    print(\"2. Downloading PDF...\")\n",
    "    paper = pipeline.download(paper_dict)\n",
    "    \n",
    "    # 3. Extract\n",
    "    print(\"3. Extracting content...\")\n",
    "    paper = pipeline.extract(paper)\n",
    "    \n",
    "    # 4. Chunk\n",
    "    print(\"4. Creating chunks...\")\n",
    "    paper = pipeline.chunk(paper)\n",
    "    \n",
    "    # 5. Embed (if embeddings available)\n",
    "    print(\"5. Creating embeddings...\")\n",
    "    try:\n",
    "        paper = pipeline.embed(paper)\n",
    "        print(\"âœ… Pipeline completed successfully!\")\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "        print(f\"Has embeddings: {paper.has_embeddings}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Embedding failed (missing dependencies): {e}\")\n",
    "        print(\"âœ… Pipeline completed (without embeddings)\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "else:\n",
    "    print(\"âŒ No papers found for pipeline demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a02b3e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "âœ… **All 4 academic providers**: arXiv, PubMed, Semantic Scholar, OpenAlex  \n",
    "âœ… **Search across all sources** with formatted and JSON output  \n",
    "âœ… **Complete processing pipeline**: search â†’ download â†’ extract â†’ chunk â†’ embed  \n",
    "âœ… **PDF extraction backends**: Marker AI, Docling, MarkItDown with auto-fallback  \n",
    "âœ… **GPU acceleration** support for faster processing  \n",
    "\n",
    "### Key Features Used:\n",
    "- Unified search interface across all providers\n",
    "- Automatic PDF downloading and text extraction\n",
    "- Intelligent text chunking for RAG\n",
    "- Vector embeddings for semantic search\n",
    "- Tabular and JSON result display\n",
    "- Error handling and graceful fallbacks\n",
    "\n",
    "Happy researching! ğŸ”¬ğŸ“š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "article",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
