{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d11c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall paperflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60996c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install paperflow[extraction-all,rag,providers] --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paperflow\n",
    "paperflow.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b22002",
   "metadata": {},
   "source": [
    "# PaperFlow: Complete Academic Paper Processing Demo\n",
    "\n",
    "PaperFlow is a unified pipeline for academic paper ingestion, extraction, and RAG (Retrieval-Augmented Generation). It allows you to:\n",
    "\n",
    "- **Search** across multiple academic sources (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- **Download** PDFs automatically\n",
    "- **Extract** text and structure from PDFs\n",
    "- **Chunk** content for RAG applications\n",
    "- **Embed** and store in vector databases\n",
    "- **Query** papers using natural language\n",
    "\n",
    "## Features\n",
    "\n",
    "- ğŸ” Multi-source search (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- ğŸ“¥ Automatic PDF downloading\n",
    "- ğŸ“„ Advanced PDF text extraction (Marker AI, Docling, MarkItDown)\n",
    "- âœ‚ï¸ Intelligent text chunking\n",
    "- ğŸ§  Vector embeddings for RAG\n",
    "- ğŸ’¾ ChromaDB integration\n",
    "- ğŸ”— LangChain compatibility\n",
    "- ğŸ“Š Tabular result display\n",
    "- ğŸ’» Command-line interface\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the latest version of PaperFlow with all optional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have CUDA GPU and want faster extraction\n",
    "PDF_DIR = './test_pdfs'\n",
    "\n",
    "# Choose PDF extraction backend:\n",
    "# - \"auto\": Try marker â†’ docling â†’ markitdown (recommended)\n",
    "# - \"marker\": High quality, best for academic papers, GPU support\n",
    "# - \"docling\": Good table/figure extraction, IBM, GPU support  \n",
    "# - \"markitdown\": Lightweight, fast, CPU only, Microsoft\n",
    "EXTRACTION_BACKEND = \"auto\"\n",
    "\n",
    "print(f\"GPU acceleration: {'Enabled' if USE_GPU else 'Disabled'}\")\n",
    "print(f\"PDF directory: {PDF_DIR}\")\n",
    "print(f\"Extraction backend: {EXTRACTION_BACKEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Create pipeline with selected backend\n",
    "pipeline = PaperPipeline(\n",
    "    gpu=USE_GPU, \n",
    "    pdf_dir=PDF_DIR,\n",
    "    extraction_backend=EXTRACTION_BACKEND\n",
    ")\n",
    "\n",
    "print(f\"âœ… Pipeline created with {pipeline._extractor.active_backend} backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a4e71",
   "metadata": {},
   "source": [
    "## 1. arXiv Provider Demo\n",
    "\n",
    "Search for computer science papers on arXiv, the preprint server for physics, mathematics, computer science, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search arXiv\n",
    "print('ğŸ” Searching arXiv for \"transformer attention\" papers...')\n",
    "results = pipeline.search('transformer attention mechanism', sources=['arxiv'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {results.total_found} papers in {results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93930069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results\n",
    "if results.papers:\n",
    "    print(\"\\nğŸ“‹ arXiv Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âŒ No papers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab82ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results in JSON format\n",
    "if results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ arXiv Search Results in JSON:\")\n",
    "    print(json.dumps(results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e688d",
   "metadata": {},
   "source": [
    "## 2. PubMed Provider Demo\n",
    "\n",
    "Search for biomedical research papers in PubMed, the premier database for biomedical literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0863a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search PubMed\n",
    "print('ğŸ” Searching PubMed for \"CRISPR gene editing\" papers...')\n",
    "pubmed_results = pipeline.search('CRISPR gene editing therapy', sources=['pubmed'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {pubmed_results.total_found} papers in {pubmed_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2331c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results\n",
    "if pubmed_results.papers:\n",
    "    print(\"\\nğŸ“‹ PubMed Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(pubmed_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3143e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results in JSON format\n",
    "if pubmed_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ PubMed Search Results in JSON:\")\n",
    "    print(json.dumps(pubmed_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8722e6",
   "metadata": {},
   "source": [
    "## 3. Semantic Scholar Provider Demo\n",
    "\n",
    "Search using Semantic Scholar's AI-powered academic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66976787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Semantic Scholar\n",
    "print('ğŸ” Searching Semantic Scholar for \"large language models\"...')\n",
    "sem_results = pipeline.search('large language models GPT', sources=['semantic_scholar'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {sem_results.total_found} papers in {sem_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52caf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results\n",
    "if sem_results.papers:\n",
    "    print(\"\\nğŸ“‹ Semantic Scholar Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(sem_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results in JSON format\n",
    "if sem_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ Semantic Scholar Search Results in JSON:\")\n",
    "    print(json.dumps(sem_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4690c84b",
   "metadata": {},
   "source": [
    "## 4. OpenAlex Provider Demo\n",
    "\n",
    "Search the OpenAlex catalog, which covers millions of scholarly works from all disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2beffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search OpenAlex\n",
    "print('ğŸ” Searching OpenAlex for \"climate change adaptation\"...')\n",
    "openalex_results = pipeline.search('climate change adaptation strategies', sources=['openalex'], max_results=3)\n",
    "\n",
    "print(f\"âœ… Found {openalex_results.total_found} papers in {openalex_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa32264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results\n",
    "if openalex_results.papers:\n",
    "    print(\"\\nğŸ“‹ OpenAlex Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(openalex_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ac010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results in JSON format\n",
    "if openalex_results.papers:\n",
    "    import json\n",
    "    print(\"ğŸ“‹ OpenAlex Search Results in JSON:\")\n",
    "    print(json.dumps(openalex_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c70f9a",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline Demo\n",
    "\n",
    "Demonstrate the full pipeline: search â†’ download â†’ extract â†’ chunk â†’ embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline demonstration\n",
    "print(\"ğŸš€ Running complete pipeline...\")\n",
    "\n",
    "# 1. Search\n",
    "print(\"1. Searching for papers...\")\n",
    "search_results = pipeline.search(\"neural networks\", sources=[\"arxiv\"], max_results=1)\n",
    "\n",
    "if search_results.papers:\n",
    "    paper_dict = search_results.papers[0]\n",
    "    \n",
    "    # 2. Download\n",
    "    print(\"2. Downloading PDF...\")\n",
    "    paper = pipeline.download(paper_dict)\n",
    "    \n",
    "    # 3. Extract\n",
    "    print(\"3. Extracting content...\")\n",
    "    paper = pipeline.extract(paper)\n",
    "    \n",
    "    # 4. Chunk\n",
    "    print(\"4. Creating chunks...\")\n",
    "    paper = pipeline.chunk(paper)\n",
    "    \n",
    "    # 5. Embed (if embeddings available)\n",
    "    print(\"5. Creating embeddings...\")\n",
    "    try:\n",
    "        paper = pipeline.embed(paper)\n",
    "        print(\"âœ… Pipeline completed successfully!\")\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "        print(f\"Has embeddings: {paper.has_embeddings}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Embedding failed (missing dependencies): {e}\")\n",
    "        print(\"âœ… Pipeline completed (without embeddings)\")\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "else:\n",
    "    print(\"âŒ No papers found for pipeline demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19924f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "âœ… **All 4 academic providers**: arXiv, PubMed, Semantic Scholar, OpenAlex  \n",
    "âœ… **Search across all sources** with formatted and JSON output  \n",
    "âœ… **Complete processing pipeline**: search â†’ download â†’ extract â†’ chunk â†’ embed  \n",
    "âœ… **PDF extraction backends**: Marker AI, Docling, MarkItDown with auto-fallback  \n",
    "âœ… **GPU acceleration** support for faster processing  \n",
    "\n",
    "### Key Features Used:\n",
    "- Unified search interface across all providers\n",
    "- Automatic PDF downloading and text extraction\n",
    "- Intelligent text chunking for RAG\n",
    "- Vector embeddings for semantic search\n",
    "- Tabular and JSON result display\n",
    "- Error handling and graceful fallbacks\n",
    "\n",
    "Happy researching! ğŸ”¬ğŸ“š"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
