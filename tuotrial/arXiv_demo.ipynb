{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec6b831a",
   "metadata": {},
   "source": [
    "# PaperFlow: Complete Academic Paper Processing Demo\n",
    "\n",
    "PaperFlow is a unified pipeline for academic paper ingestion, extraction, and RAG (Retrieval-Augmented Generation). It allows you to:\n",
    "\n",
    "- **Search** across multiple academic sources (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- **Download** PDFs automatically\n",
    "- **Extract** text and structure from PDFs\n",
    "- **Chunk** content for RAG applications\n",
    "- **Embed** and store in vector databases\n",
    "- **Query** papers using natural language\n",
    "\n",
    "## Features\n",
    "\n",
    "- üîç Multi-source search (arXiv, PubMed, Semantic Scholar, OpenAlex)\n",
    "- üì• Automatic PDF downloading\n",
    "- üìÑ Advanced PDF text extraction (Marker AI, Docling, MarkItDown)\n",
    "- ‚úÇÔ∏è Intelligent text chunking\n",
    "- üß† Vector embeddings for RAG\n",
    "- üíæ ChromaDB integration\n",
    "- üîó LangChain compatibility\n",
    "- üìä Tabular result display\n",
    "- üíª Command-line interface\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the latest version of PaperFlow with all optional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b232fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping paperflow as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall paperflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea2353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: paperflow in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (2.12.5)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (0.28.1)\n",
      "Requirement already satisfied: arxiv>=2.0.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (2.3.1)\n",
      "Requirement already satisfied: biopython>=1.80 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (1.86)\n",
      "Requirement already satisfied: requests>=2.28.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (2.32.5)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from paperflow) (0.9.0)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from arxiv>=2.0.0->paperflow) (6.0.12)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from feedparser~=6.0.10->arxiv>=2.0.0->paperflow) (1.0.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from requests>=2.28.0->paperflow) (2025.11.12)\n",
      "Requirement already satisfied: numpy in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from biopython>=1.80->paperflow) (2.3.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpx>=0.25.0->paperflow) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpx>=0.25.0->paperflow) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.0->paperflow) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic>=2.0->paperflow) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic>=2.0->paperflow) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\llmserver\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.0->paperflow) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from pydantic>=2.0->paperflow) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\llmserver\\miniconda3\\envs\\article\\lib\\site-packages (from anyio->httpx>=0.25.0->paperflow) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install paperflow[extraction-all,rag,providers] --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f859da0",
   "metadata": {},
   "source": [
    "## Demo: All Academic Providers\n",
    "\n",
    "This demo shows how to use **all academic paper providers** in PaperFlow:\n",
    "- üîç **arXiv** - Preprints and technical papers\n",
    "- üè• **PubMed** - Biomedical and life sciences research  \n",
    "- üìö **Semantic Scholar** - AI-powered academic search\n",
    "- üåê **OpenAlex** - Open catalog of scholarly works\n",
    "\n",
    "The demo covers:\n",
    "1. Search across all providers\n",
    "2. Display results in formatted tables and JSON\n",
    "3. Download PDFs from multiple sources\n",
    "4. Extract text and structure from PDFs (with optional GPU acceleration)\n",
    "5. Prepare content for RAG applications\n",
    "\n",
    "The pipeline separates download and extraction phases for better control and efficiency.\n",
    "\n",
    "## Configuration Options\n",
    "\n",
    "**GPU Support**: If you have a CUDA-compatible GPU, you can enable GPU acceleration for faster PDF extraction:\n",
    "\n",
    "```python\n",
    "USE_GPU = True  # Enable GPU acceleration\n",
    "```\n",
    "\n",
    "**Custom PDF Directory**: Specify where to save downloaded PDFs:\n",
    "\n",
    "```python\n",
    "PDF_DIR = './my_papers'  # Custom directory for PDFs\n",
    "```\n",
    "\n",
    "**PDF Extraction Backend**: Choose the best extraction method for your needs:\n",
    "\n",
    "```python\n",
    "# Options: \"auto\", \"marker\", \"docling\", \"markitdown\"\n",
    "EXTRACTION_BACKEND = \"auto\"  # Auto-select: marker ‚Üí docling ‚Üí markitdown\n",
    "# EXTRACTION_BACKEND = \"marker\"      # High quality, best for academic papers\n",
    "# EXTRACTION_BACKEND = \"docling\"     # Good table/figure extraction  \n",
    "# EXTRACTION_BACKEND = \"markitdown\"  # Lightweight, fast, CPU only\n",
    "```\n",
    "\n",
    "**Usage**:\n",
    "```python\n",
    "pipeline = PaperPipeline(\n",
    "    gpu=USE_GPU, \n",
    "    pdf_dir=PDF_DIR,\n",
    "    extraction_backend=EXTRACTION_BACKEND\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6995996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Marker AI models...\n",
      "‚úÖ Marker AI loaded\n",
      "Searching for papers on transformers...\n",
      "Found 3 papers in 551ms\n",
      "Sources: ['arxiv']\n",
      "\n",
      "+-----+----------------------------------------+------------------------------+--------+----------+--------------+\n",
      "|   # | Title                                  | Authors                      |   Year | Source   | Link/ID      |\n",
      "+=====+========================================+==============================+========+==========+==============+\n",
      "|   1 | Dilated Neighborhood Attention         | Ali Hassani, Humphrey Shi    |   2022 | arxiv    | 2209.15001v3 |\n",
      "|     | Transformer                            |                              |        |          |              |\n",
      "+-----+----------------------------------------+------------------------------+--------+----------+--------------+\n",
      "|   2 | Mask-Attention-Free Transformer for 3D | Xin Lai, Yuhui Yuan, Ruihang |   2023 | arxiv    | 2309.01692v1 |\n",
      "|     | Instance Segmentation                  | Chu et al.                   |        |          |              |\n",
      "+-----+----------------------------------------+------------------------------+--------+----------+--------------+\n",
      "|   3 | Attention Guided CAM: Visual           | Saebom Leem, Hyunseok Seo    |   2024 | arxiv    | 2402.04563v1 |\n",
      "|     | Explanations of Vision Transfor...     |                              |        |          |              |\n",
      "+-----+----------------------------------------+------------------------------+--------+----------+--------------+\n",
      "\n",
      "Downloading all papers...\n",
      "Downloading paper 1: Dilated Neighborhood Attention Transformer...\n",
      "PDF saved to: test_pdfs\\2209.15001v3.pdf\n",
      "  - PDF saved: test_pdfs\\2209.15001v3.pdf\n",
      "\n",
      "Downloading paper 2: Mask-Attention-Free Transformer for 3D Instance Se...\n",
      "PDF saved to: test_pdfs\\2309.01692v1.pdf\n",
      "  - PDF saved: test_pdfs\\2309.01692v1.pdf\n",
      "\n",
      "Downloading paper 3: Attention Guided CAM: Visual Explanations of Visio...\n",
      "PDF saved to: test_pdfs\\2402.04563v1.pdf\n",
      "  - PDF saved: test_pdfs\\2402.04563v1.pdf\n",
      "\n",
      "Extracting all papers...\n",
      "Extracting paper 1: Dilated Neighborhood Attention Transformer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Recognizing Layout: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [01:32<00:00,  5.45s/it]\n",
      "Running OCR Error Detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  4.44it/s]\n",
      "Detecting bboxes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:08<00:00,  2.93s/it]\n",
      "Recognizing Text:  14%|‚ñà‚ñç        | 51/361 [09:44<34:34,  6.69s/it]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, paper \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(papers, \u001b[32m1\u001b[39m):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExtracting paper \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpaper.metadata.title[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     paper = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m  - Sections: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(paper.sections)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Chunks: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(paper.chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NAI_Project/paperflow/src\\paperflow\\pipeline.py:259\u001b[39m, in \u001b[36mPaperPipeline.process\u001b[39m\u001b[34m(self, paper, download, extract, chunk, embed, pdf_dir)\u001b[39m\n\u001b[32m    256\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m paper\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extract \u001b[38;5;129;01mand\u001b[39;00m paper.has_pdf:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     paper = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunk:\n\u001b[32m    262\u001b[39m     paper = \u001b[38;5;28mself\u001b[39m.chunk(paper)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NAI_Project/paperflow/src\\paperflow\\pipeline.py:159\u001b[39m, in \u001b[36mPaperPipeline.extract\u001b[39m\u001b[34m(self, paper)\u001b[39m\n\u001b[32m    156\u001b[39m     paper.status = ProcessingStatus.PENDING\n\u001b[32m    157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m paper\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m sections_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_sections\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m paper.sections = [\n\u001b[32m    161\u001b[39m     Section(**section_data) \u001b[38;5;28;01mfor\u001b[39;00m section_data \u001b[38;5;129;01min\u001b[39;00m sections_dict.values()\n\u001b[32m    162\u001b[39m ]\n\u001b[32m    163\u001b[39m paper.has_sections = \u001b[38;5;28mbool\u001b[39m(paper.sections)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NAI_Project/paperflow/src\\paperflow\\processors\\marker_processor.py:75\u001b[39m, in \u001b[36mextract_sections\u001b[39m\u001b[34m(self, pdf_path)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     74\u001b[39m     rendered = \u001b[38;5;28mself\u001b[39m._converter(pdf_path)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     full_text, _, _ = \u001b[38;5;28mself\u001b[39m._text_from_rendered(rendered)\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m full_text\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\NAI_Project/paperflow/src\\paperflow\\processors\\marker_processor.py:66\u001b[39m, in \u001b[36mextract_full_text\u001b[39m\u001b[34m(self, pdf_path)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     65\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Marker AI error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28mself\u001b[39m.available = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\converters\\pdf.py:195\u001b[39m, in \u001b[36mPdfConverter.__call__\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, filepath: \u001b[38;5;28mstr\u001b[39m | io.BytesIO):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filepath_to_str(filepath) \u001b[38;5;28;01mas\u001b[39;00m temp_path:\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m         document = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m         \u001b[38;5;28mself\u001b[39m.page_count = \u001b[38;5;28mlen\u001b[39m(document.pages)\n\u001b[32m    197\u001b[39m         renderer = \u001b[38;5;28mself\u001b[39m.resolve_dependencies(\u001b[38;5;28mself\u001b[39m.renderer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\converters\\pdf.py:182\u001b[39m, in \u001b[36mPdfConverter.build_document\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m    180\u001b[39m ocr_builder = \u001b[38;5;28mself\u001b[39m.resolve_dependencies(OcrBuilder)\n\u001b[32m    181\u001b[39m provider = provider_cls(filepath, \u001b[38;5;28mself\u001b[39m.config)\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m document = \u001b[43mDocumentBuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayout_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_builder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mocr_builder\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m structure_builder_cls = \u001b[38;5;28mself\u001b[39m.resolve_dependencies(StructureBuilder)\n\u001b[32m    186\u001b[39m structure_builder_cls(document)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\document.py:36\u001b[39m, in \u001b[36mDocumentBuilder.__call__\u001b[39m\u001b[34m(self, provider, layout_builder, line_builder, ocr_builder)\u001b[39m\n\u001b[32m     34\u001b[39m line_builder(document, provider)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.disable_ocr:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     \u001b[43mocr_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m document\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\ocr.py:87\u001b[39m, in \u001b[36mOcrBuilder.__call__\u001b[39m\u001b[34m(self, document, provider)\u001b[39m\n\u001b[32m     83\u001b[39m pages_to_ocr = [page \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m document.pages \u001b[38;5;28;01mif\u001b[39;00m page.text_extraction_method == \u001b[33m'\u001b[39m\u001b[33msurya\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     84\u001b[39m ocr_page_images, block_polygons, block_ids, block_original_texts = (\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.get_ocr_images_polygons_ids(document, pages_to_ocr, provider)\n\u001b[32m     86\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mocr_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpages_to_ocr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43mocr_page_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_polygons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_original_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\marker\\builders\\ocr.py:178\u001b[39m, in \u001b[36mOcrBuilder.ocr_extraction\u001b[39m\u001b[34m(self, document, pages, images, block_polygons, block_ids, block_original_texts)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mself\u001b[39m.recognition_model.disable_tqdm = \u001b[38;5;28mself\u001b[39m.disable_tqdm\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m recognition_results: List[OCRResult] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecognition_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mocr_task_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolygons\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_polygons\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_original_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecognition_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_recognition_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisable_ocr_math\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_repeated_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdrop_repeated_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2148\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\n\u001b[32m    189\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(recognition_results) == \u001b[38;5;28mlen\u001b[39m(images) == \u001b[38;5;28mlen\u001b[39m(pages) == \u001b[38;5;28mlen\u001b[39m(block_ids), (\n\u001b[32m    192\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMismatch in OCR lengths: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(recognition_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(pages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(block_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    193\u001b[39m )\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document_page, page_recognition_result, page_block_ids, image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    195\u001b[39m     pages, recognition_results, block_ids, images\n\u001b[32m    196\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\recognition\\__init__.py:431\u001b[39m, in \u001b[36mRecognitionPredictor.__call__\u001b[39m\u001b[34m(self, images, task_names, det_predictor, detection_batch_size, recognition_batch_size, highres_images, bboxes, polygons, input_text, sort_lines, math_mode, return_words, drop_repeated_text, max_sliding_window, max_tokens, filter_tag_list)\u001b[39m\n\u001b[32m    428\u001b[39m flat[\u001b[33m\"\u001b[39m\u001b[33mtask_names\u001b[39m\u001b[33m\"\u001b[39m] = [flat[\u001b[33m\"\u001b[39m\u001b[33mtask_names\u001b[39m\u001b[33m\"\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices]\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m predicted_tokens, batch_bboxes, scores, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfoundation_predictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprediction_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mslices\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtask_names\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecognition_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmath_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrop_repeated_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lookahead_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfoundation_predictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_output_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_sliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_sliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecognizing Text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    442\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[38;5;66;03m# Get text and bboxes in structured form\u001b[39;00m\n\u001b[32m    445\u001b[39m bbox_size = \u001b[38;5;28mself\u001b[39m.bbox_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\foundation\\__init__.py:827\u001b[39m, in \u001b[36mFoundationPredictor.prediction_loop\u001b[39m\u001b[34m(self, images, input_texts, task_names, batch_size, max_tokens, max_sliding_window, math_mode, drop_repeated_tokens, max_lookahead_tokens, top_k, tqdm_desc)\u001b[39m\n\u001b[32m    825\u001b[39m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m     updated_inputs, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lookahead_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_lookahead_tokens\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    830\u001b[39m     mark_step()\n\u001b[32m    832\u001b[39m     predicted_tokens_cpu = outputs.preds.cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\foundation\\__init__.py:341\u001b[39m, in \u001b[36mFoundationPredictor.decode\u001b[39m\u001b[34m(self, current_inputs, max_lookahead_tokens)\u001b[39m\n\u001b[32m    337\u001b[39m cache_position = \u001b[38;5;28mself\u001b[39m.get_cache_position(\n\u001b[32m    338\u001b[39m     input_ids.shape[\u001b[32m1\u001b[39m], \u001b[38;5;28mself\u001b[39m.kv_cache.attention_mask, prefill=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    339\u001b[39m )\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m settings.INFERENCE_MODE():\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43membed_boxes\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m processed_output: ContinuousBatchOutput = \u001b[38;5;28mself\u001b[39m.process_outputs(\n\u001b[32m    356\u001b[39m     outputs, max_lookahead_tokens=max_lookahead_tokens\n\u001b[32m    357\u001b[39m )\n\u001b[32m    359\u001b[39m input_ids = processed_output.input_ids\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\__init__.py:468\u001b[39m, in \u001b[36mSuryaModel.forward\u001b[39m\u001b[34m(self, input_ids, image_embeddings, labels, image_tiles, grid_thw, inputs_embeds, attention_mask, position_ids, cache_position, past_key_values, output_hidden_states, output_attentions, use_cache, encoder_chunk_size, cache_idxs, num_valid_tokens, prefill, text_lengths, valid_batch_size, input_boxes, embed_boxes, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    459\u001b[39m causal_mask = \u001b[38;5;28mself\u001b[39m._update_causal_mask(\n\u001b[32m    460\u001b[39m     attention_mask,\n\u001b[32m    461\u001b[39m     inputs_embeds,\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m     output_attentions,\n\u001b[32m    465\u001b[39m )\n\u001b[32m    467\u001b[39m attention_mask = causal_mask\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logits_to_keep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\decoder\\__init__.py:504\u001b[39m, in \u001b[36mSuryaDecoderModel.forward\u001b[39m\u001b[34m(self, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, cache_idxs, num_valid_tokens, text_lengths, prefill, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_idxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_valid_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    522\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\decoder\\__init__.py:322\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, cache_idxs, num_valid_tokens, text_lengths, prefill, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m residual = hidden_states\n\u001b[32m    321\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    325\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\surya\\common\\surya\\decoder\\__init__.py:40\u001b[39m, in \u001b[36mQwen2MLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\llmserver\\miniconda3\\envs\\article\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have CUDA GPU and want faster extraction\n",
    "PDF_DIR = './test_pdfs'\n",
    "\n",
    "# Choose PDF extraction backend:\n",
    "# - \"auto\": Try marker ‚Üí docling ‚Üí markitdown (recommended)\n",
    "# - \"marker\": High quality, best for academic papers, GPU support\n",
    "# - \"docling\": Good table/figure extraction, IBM, GPU support  \n",
    "# - \"markitdown\": Lightweight, fast, CPU only, Microsoft\n",
    "EXTRACTION_BACKEND = \"auto\"\n",
    "\n",
    "print(f\"GPU acceleration: {'Enabled' if USE_GPU else 'Disabled'}\")\n",
    "print(f\"PDF directory: {PDF_DIR}\")\n",
    "print(f\"Extraction backend: {EXTRACTION_BACKEND}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Create pipeline with selected backend\n",
    "pipeline = PaperPipeline(\n",
    "    gpu=USE_GPU, \n",
    "    pdf_dir=PDF_DIR,\n",
    "    extraction_backend=EXTRACTION_BACKEND\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Pipeline created with {pipeline._extractor.active_backend} backend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0489e54f",
   "metadata": {},
   "source": [
    "## 1. arXiv Provider Demo\n",
    "\n",
    "Search for computer science papers on arXiv, the preprint server for physics, mathematics, computer science, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f501e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search arXiv\n",
    "print('üîç Searching arXiv for \"transformer attention\" papers...')\n",
    "results = pipeline.search('transformer attention mechanism', sources=['arxiv'], max_results=3)\n",
    "\n",
    "print(f\"‚úÖ Found {results.total_found} papers in {results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results\n",
    "if results.papers:\n",
    "    print(\"\\nüìã arXiv Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   arXiv ID: {paper.get('arxiv_id', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ùå No papers found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display arXiv results in JSON format\n",
    "if results.papers:\n",
    "    import json\n",
    "    print(\"üìã arXiv Search Results in JSON:\")\n",
    "    print(json.dumps(results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de67cf",
   "metadata": {},
   "source": [
    "## 2. PubMed Provider Demo\n",
    "\n",
    "Search for biomedical research papers in PubMed, the premier database for biomedical literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search PubMed\n",
    "print('üîç Searching PubMed for \"CRISPR gene editing\" papers...')\n",
    "pubmed_results = pipeline.search('CRISPR gene editing therapy', sources=['pubmed'], max_results=3)\n",
    "\n",
    "print(f\"‚úÖ Found {pubmed_results.total_found} papers in {pubmed_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80719270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results\n",
    "if pubmed_results.papers:\n",
    "    print(\"\\nüìã PubMed Search Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(pubmed_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display PubMed results in JSON format\n",
    "if pubmed_results.papers:\n",
    "    import json\n",
    "    print(\"üìã PubMed Search Results in JSON:\")\n",
    "    print(json.dumps(pubmed_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791a8f8",
   "metadata": {},
   "source": [
    "## 3. Semantic Scholar Provider Demo\n",
    "\n",
    "Search using Semantic Scholar's AI-powered academic search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search Semantic Scholar\n",
    "print('üîç Searching Semantic Scholar for \"large language models\"...')\n",
    "sem_results = pipeline.search('large language models GPT', sources=['semantic_scholar'], max_results=3)\n",
    "\n",
    "print(f\"‚úÖ Found {sem_results.total_found} papers in {sem_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac032dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results\n",
    "if sem_results.papers:\n",
    "    print(\"\\nüìã Semantic Scholar Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(sem_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800919bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Semantic Scholar results in JSON format\n",
    "if sem_results.papers:\n",
    "    import json\n",
    "    print(\"üìã Semantic Scholar Search Results in JSON:\")\n",
    "    print(json.dumps(sem_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a66d8",
   "metadata": {},
   "source": [
    "## 4. OpenAlex Provider Demo\n",
    "\n",
    "Search the OpenAlex catalog, which covers millions of scholarly works from all disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1483d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search OpenAlex\n",
    "print('üîç Searching OpenAlex for \"climate change adaptation\"...')\n",
    "openalex_results = pipeline.search('climate change adaptation strategies', sources=['openalex'], max_results=3)\n",
    "\n",
    "print(f\"‚úÖ Found {openalex_results.total_found} papers in {openalex_results.search_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88008867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results\n",
    "if openalex_results.papers:\n",
    "    print(\"\\nüìã OpenAlex Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, paper in enumerate(openalex_results.papers, 1):\n",
    "        title = paper['title'][:70] + \"...\" if len(paper['title']) > 70 else paper['title']\n",
    "        authors = \", \".join([a['name'] for a in paper.get('authors', [])[:2]])\n",
    "        if len(paper.get('authors', [])) > 2:\n",
    "            authors += \" et al.\"\n",
    "        \n",
    "        print(f\"{i}. {title}\")\n",
    "        print(f\"   Authors: {authors}\")\n",
    "        print(f\"   Year: {paper.get('year', 'N/A')}\")\n",
    "        print(f\"   DOI: {paper.get('doi', 'N/A')}\")\n",
    "        print(f\"   Citations: {paper.get('citation_count', 0)}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5fb586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display OpenAlex results in JSON format\n",
    "if openalex_results.papers:\n",
    "    import json\n",
    "    print(\"üìã OpenAlex Search Results in JSON:\")\n",
    "    print(json.dumps(openalex_results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbdbef",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline Demo\n",
    "\n",
    "Demonstrate the full pipeline: search ‚Üí download ‚Üí extract ‚Üí chunk ‚Üí embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e62b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline demonstration\n",
    "print(\"üöÄ Running complete pipeline...\")\n",
    "\n",
    "# 1. Search\n",
    "print(\"1. Searching for papers...\")\n",
    "search_results = pipeline.search(\"neural networks\", sources=[\"arxiv\"], max_results=1)\n",
    "\n",
    "if search_results.papers:\n",
    "    paper_dict = search_results.papers[0]\n",
    "    \n",
    "    # 2. Download\n",
    "    print(\"2. Downloading PDF...\")\n",
    "    paper = pipeline.download(paper_dict)\n",
    "    \n",
    "    # 3. Extract\n",
    "    print(\"3. Extracting content...\")\n",
    "    paper = pipeline.extract(paper)\n",
    "    \n",
    "    # 4. Chunk\n",
    "    print(\"4. Creating chunks...\")\n",
    "    paper = pipeline.chunk(paper)\n",
    "    \n",
    "    # 5. Embed (if embeddings available)\n",
    "    print(\"5. Creating embeddings...\")\n",
    "    try:\n",
    "        paper = pipeline.embed(paper)\n",
    "        print(\"‚úÖ Pipeline completed successfully!\")\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "        print(f\"Has embeddings: {paper.has_embeddings}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Embedding failed (missing dependencies): {e}\")\n",
    "        print(\"‚úÖ Pipeline completed (without embeddings)\")\n",
    "        \n",
    "        print(f\"\\nüìä Results:\")\n",
    "        print(f\"Title: {paper.metadata.title[:50]}...\")\n",
    "        print(f\"Sections: {len(paper.sections)}\")\n",
    "        print(f\"Chunks: {len(paper.chunks)}\")\n",
    "else:\n",
    "    print(\"‚ùå No papers found for pipeline demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f99ea",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "‚úÖ **All 4 academic providers**: arXiv, PubMed, Semantic Scholar, OpenAlex  \n",
    "‚úÖ **Search across all sources** with formatted and JSON output  \n",
    "‚úÖ **Complete processing pipeline**: search ‚Üí download ‚Üí extract ‚Üí chunk ‚Üí embed  \n",
    "‚úÖ **PDF extraction backends**: Marker AI, Docling, MarkItDown with auto-fallback  \n",
    "‚úÖ **GPU acceleration** support for faster processing  \n",
    "\n",
    "### Key Features Used:\n",
    "- Unified search interface across all providers\n",
    "- Automatic PDF downloading and text extraction\n",
    "- Intelligent text chunking for RAG\n",
    "- Vector embeddings for semantic search\n",
    "- Tabular and JSON result display\n",
    "- Error handling and graceful fallbacks\n",
    "\n",
    "Happy researching! üî¨üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6287c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Display results in JSON format\n",
    "print(\"Search Results in JSON:\")\n",
    "print(json.dumps(results.papers, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabf78a",
   "metadata": {},
   "source": [
    "## What Happens Next?\n",
    "\n",
    "After processing, you can:\n",
    "\n",
    "- **Query the papers**: Use RAG to ask questions about the content\n",
    "- **Export to LangChain**: Get LangChain documents for further processing\n",
    "- **Save to vector database**: Store embeddings for semantic search\n",
    "- **Analyze content**: Access extracted sections, chunks, and metadata\n",
    "\n",
    "## Command Line Usage\n",
    "\n",
    "You can also use paperflow from the command line:\n",
    "\n",
    "```bash\n",
    "# Install with CLI support\n",
    "pip install paperflow\n",
    "\n",
    "# Search and display results\n",
    "paperflow \"transformer attention\" --sources arxiv --max-results 5\n",
    "```\n",
    "\n",
    "## Advanced Usage\n",
    "\n",
    "For more advanced features, install optional dependencies:\n",
    "\n",
    "```bash\n",
    "pip install paperflow[all]  # Full installation\n",
    "pip install paperflow[extraction]  # PDF extraction only\n",
    "pip install paperflow[rag]  # RAG features only\n",
    "```\n",
    "\n",
    "Check out the [documentation](https://github.com/osllmai/paperflow) for more examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa65022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from paperflow import PaperPipeline\n",
    "\n",
    "# Configuration\n",
    "USE_GPU = False  # Set to True if you have CUDA GPU and want faster extraction\n",
    "PDF_DIR = './test_pdfs'\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = PaperPipeline(gpu=USE_GPU, pdf_dir=PDF_DIR)\n",
    "\n",
    "# Search for papers\n",
    "print('Searching for papers on transformers...')\n",
    "results = pipeline.search('transformer attention', sources=['arxiv'], max_results=3)\n",
    "\n",
    "# Display results\n",
    "print(results)\n",
    "print()\n",
    "\n",
    "# Process all papers\n",
    "print('Processing all papers...')\n",
    "for i, paper_meta in enumerate(results.papers, 1):\n",
    "    print(f'Processing paper {i}: {paper_meta[\"title\"][:50]}...')\n",
    "    paper = pipeline.process(paper_meta)\n",
    "    print(f'  - PDF saved: {paper.pdf_path}')\n",
    "    print(f'  - Sections: {len(paper.sections)}, Chunks: {len(paper.chunks)}')\n",
    "    print()\n",
    "\n",
    "print('All done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "article",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
